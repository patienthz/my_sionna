import torch
import torch.nn as nn

from .generate_time_channel import GenerateTimeChannel
from .apply_time_channel import ApplyTimeChannel

from .utils import time_lag_discrete_time_channel
class TimeChannel(nn.Module):
    r"""TimeChannel(channel_model, bandwidth, num_time_samples, maximum_delay_spread=3e-6, l_min=None, l_max=None, normalize_channel=False, add_awgn=True, return_channel=False, dtype=torch.complex64)

    Generate channel responses and apply them to channel inputs in the time domain.

    This class inherits from the PyTorch `nn.Module` class and can be used as a layer
    in a PyTorch model.

    The channel output consists of `num_time_samples` + `l_max` - `l_min`
    time samples, as it is the result of filtering the channel input of length
    `num_time_samples` with the time-variant channel filter  of length
    `l_max` - `l_min` + 1. In the case of a single-input single-output link and given a sequence of channel
    inputs `x_0, ..., x_{N_B}`, where `N_B` is `num_time_samples`, this
    layer outputs:

    .. math::
        y_b = \sum_{\ell = L_{\text{min}}}^{L_{\text{max}}} x_{b-\ell} \bar{h}_{b,\ell} + w_b

    where `L_{\text{min}}` corresponds `l_min`, `L_{\text{max}}` to `l_max`, `w_b` to
    the additive noise, and `\bar{h}_{b,\ell}` to the
    `\ell^{th}` tap of the `b^{th}` channel sample.
    This layer outputs `y_b` for `b` ranging from `L_{\text{min}}` to
    `N_B + L_{\text{max}} - 1`, and `x_{b}` is set to 0 for `b < 0` or `b \geq N_B`.
    The channel taps `\bar{h}_{b,\ell}` are computed assuming a sinc filter
    is used for pulse shaping and receive filtering. Therefore, given a channel impulse response
    `(a_{m}(t), \tau_{m}), 0 \leq m \leq M-1`, generated by the `channel_model`,
    the channel taps are computed as follows:

    .. math::
        \bar{h}_{b, \ell}
        = \sum_{m=0}^{M-1} a_{m}\left(\frac{b}{W}\right)
            \text{sinc}\left( \ell - W\tau_{m} \right)

    for `\ell` ranging from `l_min` to `l_max`, and where `W` is
    the `bandwidth`.

    For multiple-input multiple-output (MIMO) links, the channel output is computed for each antenna of each receiver and by summing over all the antennas of all transmitters.

    Parameters
    ----------
    channel_model : object
        An instance of a ChannelModel, such as RayleighBlockFading or UMi.

    bandwidth : float
        Bandwidth (W) [Hz]

    num_time_samples : int
        Number of time samples forming the channel input (N_B)

    maximum_delay_spread : float
        Maximum delay spread [s].
        Used to compute the default value of `l_max` if `l_max` is set to `None`. If a value is given for `l_max`, this parameter is not used.
        It defaults to 3us, which was found
        to be large enough to include most significant paths with all channel
        models included in Sionna assuming a nominal delay spread of 100ns.

    l_min : int
        Smallest time-lag for the discrete complex baseband channel (L_{\text{min}}).
        If set to `None`, defaults to the value given by `time_lag_discrete_time_channel`.

    l_max : int
        Largest time-lag for the discrete complex baseband channel (L_{\text{max}}).
        If set to `None`, it is computed from `bandwidth` and `maximum_delay_spread`
        using `time_lag_discrete_time_channel`. If it is not set to `None`,
        then the parameter `maximum_delay_spread` is not used.

    add_awgn : bool
        If set to `False`, no white Gaussian noise is added.
        Defaults to `True`.

    normalize_channel : bool
        If set to `True`, the channel is normalized over the block size
        to ensure unit average energy per time step. Defaults to `False`.

    return_channel : bool
        If set to `True`, the channel response is returned in addition to the
        channel output. Defaults to `False`.

    dtype : torch.dtype
        Complex datatype to use for internal processing and output.
        Defaults to `torch.complex64`.

    Inputs
    ------
    (x, no) or x:
        Tuple or Tensor:

    x : [batch size, num_tx, num_tx_ant, num_time_samples], torch.complex64
        Channel inputs

    no : Scalar or Tensor, torch.float
        Scalar or tensor whose shape can be broadcast to the shape of the
        channel outputs: [batch size, num_rx, num_rx_ant, num_time_samples].
        Only required if `add_awgn` is set to `True`.
        The noise power `no` is per complex dimension. If `no` is a scalar,
        noise of the same variance will be added to the outputs.
        If `no` is a tensor, it must have a shape that can be broadcast to
        the shape of the channel outputs. This allows, e.g., adding noise of
        different variance to each example in a batch. If `no` has a lower
        rank than `no` will be broadcast to the shape of the channel outputs by adding dummy dimensions after the last axis.

    Outputs
    -------
    y : [batch size, num_rx, num_rx_ant, num_time_samples + l_max - l_min], torch.complex64
        Channel outputs
        The channel output consists of `num_time_samples` + `l_max` - `l_min`
        time samples, as it is the result of filtering the channel input of length
        `num_time_samples` with the time-variant channel filter of length
        `l_max` - `l_min` + 1.

    h_time : [batch size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_time_samples + l_max - l_min, l_max - l_min + 1], torch.complex64
        (Optional) Channel responses. Returned only if `return_channel`
        is set to `True`.
        For each batch example, `num_time_samples` + `l_max` - `l_min` time
        steps of the channel realizations are generated to filter the channel input.
    """


    def __init__(self, channel_model, bandwidth, num_time_samples,
                 maximum_delay_spread=3e-6, l_min=None, l_max=None,
                 normalize_channel=False, add_awgn=True, return_channel=False,
                 dtype=torch.complex64, **kwargs) :
        super().__init__()

        # Setting l_min and l_max to default values if not given by the user
        l_min_default, l_max_default = time_lag_discrete_time_channel(bandwidth,
                                                            maximum_delay_spread)
        if l_min is None:
            l_min = l_min_default
        if l_max is None:
            l_max = l_max_default

        self._cir_sampler = channel_model
        self._bandwidth = bandwidth
        self._num_time_steps = num_time_samples
        self._l_min = l_min
        self._l_max = l_max
        self._l_tot = l_max-l_min+1
        self._normalize_channel = normalize_channel
        self._add_awgn = add_awgn
        self._return_channel = return_channel

        self._generate_channel = GenerateTimeChannel(self._cir_sampler,
                                                     self._bandwidth,
                                                     self._num_time_steps,
                                                     self._l_min,
                                                     self._l_max,
                                                     self._normalize_channel)
        self._apply_channel = ApplyTimeChannel( self._num_time_steps,
                                                self._l_tot,
                                                self._add_awgn,
                                                dtype=dtype)
        
    def forward(self, inputs):

        if self._add_awgn:
            x, no = inputs
        else:
            x = inputs
        h_time = self._generate_channel(x.shape[0])
        if self._add_awgn:
            y = self._apply_channel([x, h_time, no])
        else:
            y = self._apply_channel([x, h_time])
        
        if self._return_channel:
            return y, h_time
        else:
            return y