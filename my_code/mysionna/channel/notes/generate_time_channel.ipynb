{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sionna.utils import expand_to_rank\n",
    "def cir_to_time_channel(bandwidth, a, tau, l_min, l_max, normalize=False):\n",
    "    real_dtype = tau.dtype\n",
    "\n",
    "    if len(tau.shape) == 4:\n",
    "        # Expand dims to broadcast with h. Add the following dimensions:\n",
    "        #  - number of rx antennas (2)\n",
    "        #  - number of tx antennas (4)\n",
    "        tau = tf.expand_dims(tf.expand_dims(tau, axis=2), axis=4)\n",
    "        # Broadcast is not supported by TF for such high rank tensors.\n",
    "        # We therefore do part of it manually\n",
    "        tau = tf.tile(tau, [1, 1, 1, 1, a.shape[4], 1])\n",
    "\n",
    "    # Add a time samples dimension for broadcasting\n",
    "    tau = tf.expand_dims(tau, axis=6)\n",
    "\n",
    "    # Time lags for which to compute the channel taps\n",
    "    l = tf.range(l_min, l_max+1, dtype=real_dtype)\n",
    "\n",
    "    # Bring tau and l to broadcastable shapes\n",
    "    tau = tf.expand_dims(tau, axis=-1)\n",
    "    l = expand_to_rank(l, tau.shape.rank, axis=0)\n",
    "\n",
    "    # sinc pulse shaping\n",
    "    g = tf.experimental.numpy.sinc(l-tau*bandwidth)\n",
    "    g = tf.complex(g, tf.constant(0., real_dtype))\n",
    "    a = tf.expand_dims(a, axis=-1)\n",
    "\n",
    "    # For every tap, sum the sinc-weighted coefficients\n",
    "    hm = tf.reduce_sum(a*g, axis=-3)\n",
    "\n",
    "    if normalize:\n",
    "        # Normalization is performed such that for each batch example and\n",
    "        # link the energy per block is one.\n",
    "        # The total energy of a channel response is the sum of the squared\n",
    "        # norm over the channel taps.\n",
    "        # Average over block size, RX antennas, and TX antennas\n",
    "        c = tf.reduce_mean(tf.reduce_sum(tf.square(tf.abs(hm)),\n",
    "                                         axis=6, keepdims=True),\n",
    "                           axis=(2,4,5), keepdims=True)\n",
    "        c = tf.complex(tf.sqrt(c), tf.constant(0., real_dtype))\n",
    "        hm = tf.math.divide_no_nan(hm, c)\n",
    "\n",
    "    return hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[[[[-0.0010297 -0.01116179j  0.04673905+0.02841946j\n",
      "       -0.12742408-0.13641776j  0.20179068+0.20753935j\n",
      "       -0.1098018 -0.12750523j -0.07490576-0.06217125j\n",
      "        0.01046753-0.02048977j  0.01500362+0.04721779j\n",
      "       -0.09453862-0.1185082j   0.09851032+0.10335578j\n",
      "       -0.12903695-0.10412582j]\n",
      "      [ 0.21346247+0.27125633j -0.17321755-0.29813966j\n",
      "        0.24720478+0.30445552j -0.18566336-0.33407715j\n",
      "        0.06421079+0.31312802j -0.13005596-0.37832937j\n",
      "        0.22443095+0.37106553j -0.1978377 -0.28699064j\n",
      "        0.13127004+0.33030245j -0.26518646-0.3003759j\n",
      "        0.2386253 +0.30928183j]\n",
      "      [ 0.12565194+0.05868819j -0.07910819-0.05132939j\n",
      "        0.11684381+0.10935003j -0.05461621-0.11052839j\n",
      "        0.11637517+0.2155125j  -0.11721931-0.05677493j\n",
      "        0.16700381+0.1080543j  -0.1117426 -0.05742691j\n",
      "        0.04396446+0.05841747j -0.12992199-0.09754615j\n",
      "        0.09575476+0.08817555j]\n",
      "      [-0.04500984+0.01795548j  0.02107948+0.07768019j\n",
      "       -0.03325387-0.17327406j -0.0046881 +0.28763974j\n",
      "       -0.10325332-0.15053502j  0.05832467-0.13536519j\n",
      "       -0.07736251+0.05547277j  0.0298124 -0.01930415j\n",
      "        0.01071429-0.10734311j  0.03923555+0.13317358j\n",
      "       -0.02314523-0.20764738j]\n",
      "      [-0.16375272+0.00120166j  0.19099967-0.04762601j\n",
      "       -0.2631018 +0.02316968j  0.23544668-0.02664166j\n",
      "       -0.12387626+0.02384941j  0.03645539+0.01728847j\n",
      "       -0.13807143-0.02608956j  0.10785959+0.03968241j\n",
      "       -0.08498042-0.02810533j  0.2720354 -0.02753772j\n",
      "       -0.3108225 +0.0768574j ]\n",
      "      [ 0.29793692-0.24337588j -0.25330147+0.2024355j\n",
      "        0.34846252-0.30337617j -0.35211805+0.3280146j\n",
      "        0.30407432-0.13907647j -0.34369835+0.23842877j\n",
      "        0.4157913 -0.31341082j -0.3499159 +0.31242043j\n",
      "        0.35276645-0.33385816j -0.33986256+0.2924794j\n",
      "        0.28206712-0.23595463j]\n",
      "      [-0.23628172+0.16071159j  0.2486014 -0.12792027j\n",
      "       -0.2524358 +0.19493333j  0.28000036-0.16025177j\n",
      "       -0.20104782+0.21739736j  0.32656693-0.15239118j\n",
      "       -0.3153735 +0.22184776j  0.26554933-0.15646258j\n",
      "       -0.30280834+0.11918114j  0.25062236-0.1964977j\n",
      "       -0.24609675+0.16819839j]\n",
      "      [-0.14087796-0.03018736j  0.20686576+0.04182645j\n",
      "       -0.2618162 -0.08887804j  0.31072208+0.13619712j\n",
      "       -0.11082072+0.0701426j   0.05905473-0.0299196j\n",
      "       -0.11891589-0.00436435j  0.12419058+0.06438602j\n",
      "       -0.18507113-0.11188584j  0.25270092+0.0771305j\n",
      "       -0.30907592-0.07761008j]\n",
      "      [ 0.4178382 +0.05037823j -0.40900767+0.0075273j\n",
      "        0.5589483 +0.00103904j -0.6033215 +0.0093692j\n",
      "        0.2534048 -0.00436598j -0.3625026 -0.1133017j\n",
      "        0.49139923+0.10297514j -0.47876707-0.10211273j\n",
      "        0.5276528 +0.08402097j -0.5466474 -0.00120251j\n",
      "        0.5166383 -0.06156128j]\n",
      "      [ 0.10097726-0.21958774j -0.11297429+0.22949211j\n",
      "        0.11640292-0.330984j   -0.14802659+0.39381027j\n",
      "        0.03372496-0.08278453j -0.13088034+0.15965362j\n",
      "        0.12255409-0.23958807j -0.12814169+0.27519545j\n",
      "        0.16459721-0.34156346j -0.11261467+0.31467775j\n",
      "        0.11242139-0.30423605j]]]]]]], shape=(1, 1, 1, 1, 1, 10, 11), dtype=complex64)\n",
      "(1, 1, 1, 1, 1, 10, 11)\n",
      "<dtype: 'complex64'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from my_code.mysionna.constants import GLOBAL_SEED_NUMBER\n",
    "\n",
    "def test_cir_to_time_channel():\n",
    "    #设置种子\n",
    "    tf.random.set_seed(GLOBAL_SEED_NUMBER)\n",
    "\n",
    "    # 示例参数\n",
    "    bandwidth = 20e6  # 20 MHz\n",
    "\n",
    "    # 生成模拟数据\n",
    "    batch_size = 1\n",
    "    num_rx = 1\n",
    "    num_rx_ant = 1\n",
    "    num_tx = 1\n",
    "    num_tx_ant = 1\n",
    "    num_paths = 5\n",
    "    num_time_steps = 10\n",
    "\n",
    "    # 随机生成路径系数（复数）\n",
    "    a = tf.complex(tf.random.normal([batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps]),\n",
    "                tf.random.normal([batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps]))\n",
    "\n",
    "    # 随机生成路径延迟（实数）\n",
    "    tau = tf.random.uniform([batch_size, num_rx, num_tx, num_paths])\n",
    "\n",
    "    # 时间抽头范围\n",
    "    l_min = -5\n",
    "    l_max = 5\n",
    "\n",
    "    # 调用函数\n",
    "    hm = cir_to_time_channel(bandwidth, a, tau, l_min, l_max, normalize=True)\n",
    "\n",
    "    print(hm)\n",
    "    print(hm.shape)\n",
    "    print(hm.dtype)\n",
    "test_cir_to_time_channel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from my_code.mysionna.constants import GLOBAL_SEED_NUMBER\n",
    "from my_code.mysionna.utils import expand_to_rank\n",
    "def cir_to_time_channel(bandwidth, a, tau, l_min, l_max, normalize=False):\n",
    "\n",
    "    # tau = torch.tensor(tau)\n",
    "    real_dtype = tau.dtype\n",
    "\n",
    "    if len(tau.shape) == 4:\n",
    "        # Expand dims to broadcast with h. Add the following dimensions:\n",
    "        #  - number of rx antennas (2)\n",
    "        #  - number of tx antennas (4)\n",
    "        tau = torch.unsqueeze(torch.unsqueeze(tau,dim=2),dim=4)\n",
    "        # We therefore do part of it manually\n",
    "        tau = tau.repeat(1,1,1,1,a.shape[4],1)\n",
    "    \n",
    "    # Add a time samples dimension for broadcasting\n",
    "    tau = torch.unsqueeze(tau, dim=6)\n",
    "\n",
    "    # Time lags for which to compute the channel taps\n",
    "    l = torch.arange(l_min, l_max+1, dtype=real_dtype)\n",
    "\n",
    "    # Bring tau and l to broadcastable shapes\n",
    "    tau = torch.unsqueeze(tau, dim=-1)\n",
    "    l = expand_to_rank(l, tau.dim(),axis=0)\n",
    "\n",
    "    # sinc pulse shaping\n",
    "    g = torch.sinc(l-tau*bandwidth)\n",
    "    g = torch.complex(g, torch.tensor(0.,dtype=real_dtype))\n",
    "    a = torch.unsqueeze(a, dim=-1)\n",
    "\n",
    "    # For every tap, sum the sinc-weighted coefficients\n",
    "    hm = torch.sum(a*g, dim=-3)\n",
    "\n",
    "    if normalize:\n",
    "        # Normalization is performed such that for each batch example and\n",
    "        # link the energy per block is one.\n",
    "        # The total energy of a channel response is the sum of the squared\n",
    "        # norm over the channel taps.\n",
    "        # Average over block size, RX antennas, and TX antennas\n",
    "        c = torch.mean(torch.sum(torch.square(torch.abs(hm)),\n",
    "                                dim=6, \n",
    "                                keepdim=True),\n",
    "                            dim=(2,4,5),\n",
    "                            keepdim=True\n",
    "                       )\n",
    "        c = torch.complex(torch.sqrt(c), torch.tensor(0.,dtype=real_dtype))\n",
    "        hm = torch.divide(hm, c+1e-10)\n",
    "    \n",
    "    return hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[[[-0.0010-0.0112j,  0.0467+0.0284j, -0.1274-0.1364j,\n",
      "               0.2018+0.2075j, -0.1098-0.1275j, -0.0749-0.0622j,\n",
      "               0.0105-0.0205j,  0.0150+0.0472j, -0.0945-0.1185j,\n",
      "               0.0985+0.1033j, -0.1290-0.1041j],\n",
      "             [ 0.2134+0.2712j, -0.1732-0.2981j,  0.2472+0.3044j,\n",
      "              -0.1856-0.3340j,  0.0642+0.3131j, -0.1300-0.3783j,\n",
      "               0.2244+0.3710j, -0.1978-0.2870j,  0.1313+0.3303j,\n",
      "              -0.2651-0.3003j,  0.2386+0.3092j],\n",
      "             [ 0.1256+0.0587j, -0.0791-0.0513j,  0.1168+0.1093j,\n",
      "              -0.0546-0.1105j,  0.1164+0.2155j, -0.1172-0.0568j,\n",
      "               0.1670+0.1080j, -0.1117-0.0574j,  0.0440+0.0584j,\n",
      "              -0.1299-0.0975j,  0.0957+0.0882j],\n",
      "             [-0.0450+0.0180j,  0.0211+0.0777j, -0.0332-0.1732j,\n",
      "              -0.0047+0.2876j, -0.1032-0.1505j,  0.0583-0.1353j,\n",
      "              -0.0774+0.0555j,  0.0298-0.0193j,  0.0107-0.1073j,\n",
      "               0.0392+0.1332j, -0.0231-0.2076j],\n",
      "             [-0.1637+0.0012j,  0.1910-0.0476j, -0.2631+0.0232j,\n",
      "               0.2354-0.0266j, -0.1239+0.0238j,  0.0365+0.0173j,\n",
      "              -0.1381-0.0261j,  0.1078+0.0397j, -0.0850-0.0281j,\n",
      "               0.2720-0.0275j, -0.3108+0.0768j],\n",
      "             [ 0.2979-0.2433j, -0.2533+0.2024j,  0.3484-0.3033j,\n",
      "              -0.3521+0.3280j,  0.3040-0.1391j, -0.3437+0.2384j,\n",
      "               0.4157-0.3134j, -0.3499+0.3124j,  0.3527-0.3338j,\n",
      "              -0.3398+0.2924j,  0.2820-0.2359j],\n",
      "             [-0.2362+0.1607j,  0.2486-0.1279j, -0.2524+0.1949j,\n",
      "               0.2800-0.1602j, -0.2010+0.2174j,  0.3265-0.1524j,\n",
      "              -0.3153+0.2218j,  0.2655-0.1564j, -0.3028+0.1192j,\n",
      "               0.2506-0.1965j, -0.2461+0.1682j],\n",
      "             [-0.1409-0.0302j,  0.2068+0.0418j, -0.2618-0.0889j,\n",
      "               0.3107+0.1362j, -0.1108+0.0701j,  0.0590-0.0299j,\n",
      "              -0.1189-0.0044j,  0.1242+0.0644j, -0.1850-0.1119j,\n",
      "               0.2527+0.0771j, -0.3090-0.0776j],\n",
      "             [ 0.4178+0.0504j, -0.4090+0.0075j,  0.5589+0.0010j,\n",
      "              -0.6032+0.0094j,  0.2534-0.0044j, -0.3625-0.1133j,\n",
      "               0.4913+0.1030j, -0.4787-0.1021j,  0.5276+0.0840j,\n",
      "              -0.5466-0.0012j,  0.5166-0.0616j],\n",
      "             [ 0.1010-0.2196j, -0.1130+0.2295j,  0.1164-0.3309j,\n",
      "              -0.1480+0.3938j,  0.0337-0.0828j, -0.1309+0.1596j,\n",
      "               0.1225-0.2396j, -0.1281+0.2752j,  0.1646-0.3415j,\n",
      "              -0.1126+0.3146j,  0.1124-0.3042j]]]]]]])\n",
      "torch.Size([1, 1, 1, 1, 1, 10, 11])\n",
      "torch.complex64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def test_cir_to_time_channel():\n",
    "    #设置种子\n",
    "    tf.random.set_seed(GLOBAL_SEED_NUMBER)\n",
    "\n",
    "    # 示例参数\n",
    "    bandwidth = 20e6  # 20 MHz\n",
    "\n",
    "    # 生成模拟数据\n",
    "    batch_size = 1\n",
    "    num_rx = 1\n",
    "    num_rx_ant = 1\n",
    "    num_tx = 1\n",
    "    num_tx_ant = 1\n",
    "    num_paths = 5\n",
    "    num_time_steps = 10\n",
    "\n",
    "    # 随机生成路径系数（复数）\n",
    "    a_tf = tf.complex(tf.random.normal([batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps]),\n",
    "                tf.random.normal([batch_size, num_rx, num_rx_ant, num_tx, num_tx_ant, num_paths, num_time_steps]))\n",
    "\n",
    "    # 随机生成路径延迟（实数）\n",
    "    tau_tf = tf.random.uniform([batch_size, num_rx, num_tx, num_paths])\n",
    "\n",
    "    #将 TensorFlow 数据转换为 NumPy 数组\n",
    "    a_np = a_tf.numpy()\n",
    "    tau_np = tau_tf.numpy()\n",
    "    # 将 NumPy 数组转换为 PyTorch 张量\n",
    "    a = torch.from_numpy(a_np)\n",
    "    tau = torch.from_numpy(tau_np)\n",
    "    # 时间抽头范围\n",
    "    l_min = -5\n",
    "    l_max = 5\n",
    "\n",
    "    # 调用函数\n",
    "    hm = cir_to_time_channel(bandwidth, a, tau, l_min, l_max, normalize=True)\n",
    "\n",
    "    print(hm)\n",
    "    print(hm.shape)\n",
    "    print(hm.dtype)\n",
    "test_cir_to_time_channel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mysionna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
