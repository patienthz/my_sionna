{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 11:31:32.898536: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-29 11:31:32.898571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-29 11:31:32.916038: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-29 11:31:32.954766: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-29 11:31:34.425510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from sionna.utils import expand_to_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BinaryMemorylessChannel类\n",
    "\n",
    "`BinaryMemorylessChannel`类是一个离散的二进制无记忆通道，具有（可能的）不对称比特翻转概率。\n",
    "\n",
    "输入的比特以概率 \\( p_\\text{b,0} \\) 和 \\( p_\\text{b,1} \\) 分别翻转。\n",
    "\n",
    "这个层支持二进制输入（\\( x \\in \\{0, 1\\} \\)）和双极性输入（\\( x \\in \\{-1, 1\\} \\)）。\n",
    "\n",
    "如果激活，该通道直接返回对数似然比（LLRs），定义如下：\n",
    "\n",
    "\\[\n",
    "\\ell =\n",
    "\\begin{cases}\n",
    "    \\operatorname{log} \\frac{p_{b,1}}{1-p_{b,0}}, \\qquad \\text{如果} \\, y=0 \\\\\n",
    "    \\operatorname{log} \\frac{1-p_{b,1}}{p_{b,0}}, \\qquad \\text{如果} \\, y=1 \\\\\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "错误概率 \\( p_\\text{b}\\) 可以是标量或张量（可广播到输入的形状）。这允许每个位位置有不同的擦除概率。在任何情况下，它的最后一个维度的长度必须为2，并被解释为 \\( p_\\text{b,0} \\) 和 \\( p_\\text{b,1} \\)。\n",
    "\n",
    "这个类继承自Keras的`Layer`类，可以在Keras模型中用作层。\n",
    "\n",
    "## 参数\n",
    "\n",
    "- `return_llrs`：布尔类型，默认值为 `False`。如果设置为 `True`，则该层返回基于 `pb` 的对数似然比（LLRs）而不是二进制值。\n",
    "\n",
    "- `bipolar_input`：布尔类型，默认值为 `False`。如果设置为 `True`，则期望的输入是 \\( \\{-1,1\\} \\) 而不是 \\( \\{0,1\\} \\)。\n",
    "\n",
    "- `llr_max`：`tf.float`类型，默认值为100。定义LLRs的裁剪值。\n",
    "\n",
    "- `dtype`：`tf.DType`类型，定义内部计算和输出的类型。默认值为 `tf.float32`。\n",
    "\n",
    "## 输入\n",
    "\n",
    "- `(x, pb)`：\n",
    "  - `x`：形状为 [...,n] 的 `tf.float32` 类型的输入序列，包含二进制值 \\( \\{0,1\\} \\) 或 \\( \\{-1,1\\} \\)。\n",
    "  - `pb`：形状为 [...,2] 的 `tf.float32` 类型的错误概率。可以是两个标量的元组或任何可以广播到 `x` 形状的形状。它有一个额外的最后维度，解释为 \\( p_\\text{b,0} \\) 和 \\( p_\\text{b,1} \\)。\n",
    "\n",
    "## 输出\n",
    "\n",
    "- 形状为 [...,n] 的 `tf.float32` 类型的输出序列，长度与输入 `x` 相同。如果 `return_llrs` 为 `False`，则输出是三元的，其中 `-1` 和 `0` 分别表示二进制和双极性输入的擦除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryMemorylessChannel(Layer):\n",
    "    def __init__(self, return_llrs=False, bipolar_input=False, llr_max=100.,dtype=tf.float32, **kwargs):\n",
    "\n",
    "        super().__init__(dtype=dtype,**kwargs)\n",
    "\n",
    "        assert isinstance(return_llrs, bool), \"return_llrs must be bool.\"\n",
    "        self._return_llrs = return_llrs\n",
    "\n",
    "        assert isinstance(bipolar_input, bool), \"bipolar_input must be bool.\"\n",
    "        self._bipolar_input = bipolar_input\n",
    "\n",
    "        assert llr_max>=0., \"llr_max must be a positive scalar value.\"\n",
    "        self._llr_max = tf.cast(llr_max, dtype=self.dtype)\n",
    "\n",
    "        if self._return_llrs:\n",
    "            assert dtype in (tf.float16, tf.float32, tf.float64),\\\n",
    "                \"LLR outputs require non-integer dtypes.\"\n",
    "        else:\n",
    "            if self._bipolar_input:\n",
    "                assert dtype in (tf.float16, tf.float32, tf.float64,\n",
    "                    tf.int8, tf.int16, tf.int32, tf.int64),\\\n",
    "                    \"Only, signed dtypes are supported for bipolar inputs.\"\n",
    "            else:\n",
    "                assert dtype in (tf.float16, tf.float32, tf.float64,\n",
    "                    tf.uint8, tf.uint16, tf.uint32, tf.uint64,\n",
    "                    tf.int8, tf.int16, tf.int32, tf.int64),\\\n",
    "                    \"Only, real-valued dtypes are supported.\"\n",
    "\n",
    "        self._check_input = True # check input for consistency (i.e., binary)\n",
    "\n",
    "        self._eps = 1e-9 # small additional term for numerical stability\n",
    "        self._temperature = tf.constant(0.1, tf.float32) # for Gumble-softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个`BinaryMemorylessChannel`类的构造函数中，有多个步骤来初始化和验证输入参数。下面是对每一行代码的详细解释：\n",
    "\n",
    "```python\n",
    "def __init__(self, return_llrs=False, bipolar_input=False, llr_max=100., dtype=tf.float32, **kwargs):\n",
    "```\n",
    "\n",
    "- 这是类的构造函数（初始化方法），用于在创建类的实例时设置初始值。\n",
    "- `return_llrs`: 是否返回对数似然比 (LLRs)，默认值为`False`。\n",
    "- `bipolar_input`: 输入是否为双极性的（{-1, 1}），默认值为`False`。\n",
    "- `llr_max`: LLRs的最大剪辑值，默认值为`100`。\n",
    "- `dtype`: 数据类型，默认值为`tf.float32`。\n",
    "- `**kwargs`: 其他可选参数。\n",
    "\n",
    "```python\n",
    "super().__init__(dtype=dtype, **kwargs)\n",
    "```\n",
    "\n",
    "- 调用父类`Layer`的初始化方法，并传递数据类型和其他参数。\n",
    "\n",
    "```python\n",
    "assert isinstance(return_llrs, bool), \"return_llrs must be bool.\"\n",
    "self._return_llrs = return_llrs\n",
    "```\n",
    "\n",
    "- 检查`return_llrs`是否为布尔值，如果不是，抛出一个断言错误。\n",
    "- 将`return_llrs`保存为实例变量`self._return_llrs`。\n",
    "\n",
    "```python\n",
    "assert isinstance(bipolar_input, bool), \"bipolar_input must be bool.\"\n",
    "self._bipolar_input = bipolar_input\n",
    "```\n",
    "\n",
    "- 检查`bipolar_input`是否为布尔值，如果不是，抛出一个断言错误。\n",
    "- 将`bipolar_input`保存为实例变量`self._bipolar_input`。\n",
    "\n",
    "```python\n",
    "assert llr_max >= 0., \"llr_max must be a positive scalar value.\"\n",
    "self._llr_max = tf.cast(llr_max, dtype=self.dtype)\n",
    "```\n",
    "\n",
    "- 检查`llr_max`是否为非负值，如果不是，抛出一个断言错误。\n",
    "- 将`llr_max`转换为指定的数据类型并保存为实例变量`self._llr_max`。\n",
    "\n",
    "```python\n",
    "if self._return_llrs:\n",
    "    assert dtype in (tf.float16, tf.float32, tf.float64), \"LLR outputs require non-integer dtypes.\"\n",
    "```\n",
    "\n",
    "- 如果返回LLRs，则检查数据类型是否为`tf.float16`、`tf.float32`或`tf.float64`中的一种。如果不是，抛出一个断言错误。\n",
    "\n",
    "```python\n",
    "else:\n",
    "    if self._bipolar_input:\n",
    "        assert dtype in (tf.float16, tf.float32, tf.float64, tf.int8, tf.int16, tf.int32, tf.int64),\\\n",
    "            \"Only, signed dtypes are supported for bipolar inputs.\"\n",
    "```\n",
    "\n",
    "- 如果不返回LLRs，且输入为双极性，则检查数据类型是否为`tf.float16`、`tf.float32`、`tf.float64`、`tf.int8`、`tf.int16`、`tf.int32`或`tf.int64`中的一种。如果不是，抛出一个断言错误。\n",
    "\n",
    "```python\n",
    "    else:\n",
    "        assert dtype in (tf.float16, tf.float32, tf.float64, tf.uint8, tf.uint16, tf.uint32, tf.uint64, tf.int8, tf.int16, tf.int32, tf.int64),\\\n",
    "            \"Only, real-valued dtypes are supported.\"\n",
    "```\n",
    "\n",
    "- 如果不返回LLRs，且输入不是双极性，则检查数据类型是否为`tf.float16`、`tf.float32`、`tf.float64`、`tf.uint8`、`tf.uint16`、`tf.uint32`、`tf.uint64`、`tf.int8`、`tf.int16`、`tf.int32`或`tf.int64`中的一种。如果不是，抛出一个断言错误。\n",
    "\n",
    "```python\n",
    "self._check_input = True # check input for consistency (i.e., binary)\n",
    "```\n",
    "\n",
    "- 设置一个实例变量`self._check_input`为`True`，用于在后续方法中检查输入的一致性（即是否为二进制）。\n",
    "\n",
    "```python\n",
    "self._eps = 1e-9 # small additional term for numerical stability\n",
    "```\n",
    "\n",
    "- 设置一个实例变量`self._eps`为`1e-9`，用于数值稳定性的小附加项。\n",
    "\n",
    "```python\n",
    "self._temperature = tf.constant(0.1, tf.float32) # for Gumble-softmax\n",
    "```\n",
    "\n",
    "- 设置一个实例变量`self._temperature`为常量`0.1`，用于Gumbel-Softmax。\n",
    "\n",
    "### 详细解释\n",
    "\n",
    "通过这些初始化步骤，`BinaryMemorylessChannel`类的构造函数确保了所有输入参数都是有效的，并根据这些参数设置了一些实例变量，用于后续的计算和操作。这些检查和设置有助于提高代码的健壮性和可维护性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 详细解释 `Gumbel-Softmax` 机制\n",
    "\n",
    "`Gumbel-Softmax` 是一种用于从离散分布中采样的近似方法，特别是在需要梯度信息的情况下。这在机器学习和深度学习中非常有用，例如在生成模型或离散动作空间的强化学习中。\n",
    "\n",
    "#### Gumbel 分布\n",
    "\n",
    "首先，`Gumbel-Softmax` 的基础是 Gumbel 分布。Gumbel 分布是一种极值分布，通常用于建模最大值或最小值的分布。其概率密度函数 (PDF) 为：\n",
    "\n",
    "$$\n",
    "f(x) = e^{-(x + e^{-x})}\n",
    "$$\n",
    "\n",
    "Gumbel 分布的一个重要性质是其用于最大值采样时的重参数化技巧。\n",
    "\n",
    "#### Gumbel-Max 采样\n",
    "\n",
    "给定一个类别分布 \\(\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_k)\\)，我们可以使用 Gumbel-Max 采样方法来从中采样一个类别：\n",
    "\n",
    "$$\n",
    "\\text{sample} = \\arg\\max_i \\left( \\log(\\pi_i) + g_i \\right)\n",
    "$$\n",
    "\n",
    "其中 $(g_i)$是从 Gumbel(0,1) 分布中采样的随机变量。\n",
    "\n",
    "#### Gumbel-Softmax\n",
    "\n",
    "然而，Gumbel-Max 采样是离散的，不适合梯度优化。因此，Gumbel-Softmax 引入了一个温度参数 $(\\tau)$，并将采样过程近似为：\n",
    "\n",
    "$$\n",
    "y_i = \\frac{\\exp\\left((\\log(\\pi_i) + g_i) / \\tau \\right)}{\\sum_{j=1}^k \\exp\\left((\\log(\\pi_j) + g_j) / \\tau \\right)}\n",
    "$$\n",
    "\n",
    "当 $(\\tau \\to 0)$ 时，Gumbel-Softmax 分布趋近于离散分布。当 $(\\tau \\to \\infty)$ 时，分布变得更加均匀。通过调整 $(\\tau)$，我们可以控制采样的离散程度。\n",
    "\n",
    "### `Gumbel-Softmax` 在 `BinaryMemorylessChannel` 中的应用\n",
    "\n",
    "在 `BinaryMemorylessChannel` 类的初始化过程中，设置 `self._temperature = tf.constant(0.1, tf.float32)` 是为了在某些计算中使用 `Gumbel-Softmax` 机制。具体来说，`Gumbel-Softmax` 可以用于模拟离散的二元信道，同时保持对梯度的支持。这对于神经网络的训练非常重要，因为它允许使用反向传播来优化参数。\n",
    "\n",
    "### 总结\n",
    "\n",
    "通过上述初始化和检查步骤，`BinaryMemorylessChannel` 类确保输入参数有效，并设置实例变量以支持后续的计算和操作。其中，`Gumbel-Softmax` 机制的引入，使得在处理离散信道采样时，能够有效地进行梯度优化，从而提高模型的性能和训练效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BinaryMemorylessChannel (torch version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "from my_code.mysionna.channel.torch_version.utils import expand_to_rank\n",
    "\n",
    "\n",
    "class CustomOperations:\n",
    "    \n",
    "    class CustomXOR(Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, a, b):\n",
    "            ctx.save_for_backward(a, b)\n",
    "            if a.dtype in (torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64):\n",
    "                z = (a + b) % 2\n",
    "            else:\n",
    "                z = torch.abs(a - b)\n",
    "            return z\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            return grad_output, grad_output\n",
    "    # STEBinarizer（Straight-Through Estimator Binarizer）是一种在神经网络中用于处理二值化操作的技术。\n",
    "    # STE代表Straight-Through Estimator，它是一种用于在反向传播中处理不可微操作的技术。\n",
    "    class STEBinarizer(Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, x):\n",
    "            ctx.save_for_backward(x)\n",
    "            z = torch.where(x < 0.5, torch.tensor(0.0, device=x.device), torch.tensor(1.0, device=x.device))\n",
    "            return z\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            return grad_output\n",
    "    \n",
    "    class SampleErrors(torch.nn.Module):\n",
    "        def __init__(self, eps=1e-10, temperature=1.0):\n",
    "            super().__init__()\n",
    "            self._eps = eps\n",
    "            self._temperature = temperature\n",
    "\n",
    "        def forward(self, pb, shape):\n",
    "            u1 = torch.rand(shape, dtype=torch.float32)\n",
    "            u2 = torch.rand(shape, dtype=torch.float32)\n",
    "            u = torch.stack((u1, u2), dim=-1)\n",
    "\n",
    "            # 采样Gumbel分布\n",
    "            q = -torch.log(-torch.log(u + self._eps) + self._eps)\n",
    "            p = torch.stack((pb, 1 - pb), dim=-1).unsqueeze(1).expand(shape[0], shape[1], 2)\n",
    "            a = (torch.log(p + self._eps) + q) / self._temperature\n",
    "\n",
    "            # 应用softmax\n",
    "            e_cat = F.softmax(a, dim=-1)\n",
    "\n",
    "            # 通过直通估计器对最终值进行二值化\n",
    "            return CustomOperations.STEBinarizer.apply(e_cat[..., 0])\n",
    "\n",
    "class BinaryMemorylessChannel(nn.Module):\n",
    "    def __init__(self, return_llrs=False, bipolar_input=False, llr_max=100., dtype=torch.float32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        assert isinstance(return_llrs, bool), \"return_llrs must be bool.\"\n",
    "        self.return_llrs = return_llrs\n",
    "\n",
    "        assert isinstance(bipolar_input, bool), \"bipolar_input must be bool.\"\n",
    "        self.bipolar_input = bipolar_input\n",
    "\n",
    "        assert llr_max >= 0., \"llr_max must be a positive scalar value.\"\n",
    "        self.llr_max = llr_max\n",
    "        self.dtype = dtype\n",
    "\n",
    "        if self.return_llrs:\n",
    "            assert dtype in (torch.float16, torch.float32, torch.float64), \\\n",
    "                \"LLR outputs require non-integer dtypes.\"\n",
    "        else:\n",
    "            if self.bipolar_input:\n",
    "                assert dtype in (torch.float16, torch.float32, torch.float64,\n",
    "                                 torch.int8, torch.int16, torch.int32, torch.int64), \\\n",
    "                    \"Only signed dtypes are supported for bipolar inputs.\"\n",
    "            else:\n",
    "                assert dtype in (torch.float16, torch.float32, torch.float64,\n",
    "                                 torch.uint8, torch.uint16, torch.uint32, torch.uint64,\n",
    "                                 torch.int8, torch.int16, torch.int32, torch.int64), \\\n",
    "                    \"Only real-valued dtypes are supported.\"\n",
    "\n",
    "        self.check_input = True  # check input for consistency (i.e., binary)\n",
    "\n",
    "        self.eps = 1e-9  # small additional term for numerical stability\n",
    "        self.temperature = torch.tensor(0.1, dtype=torch.float32)  # for Gumble-softmax\n",
    "\n",
    "    @property\n",
    "    def llr_max(self):\n",
    "        \"\"\"Maximum value used for LLR calculations.\"\"\"\n",
    "        return self._llr_max\n",
    "\n",
    "    @llr_max.setter\n",
    "    def llr_max(self, value):\n",
    "        \"\"\"Maximum value used for LLR calculations.\"\"\"\n",
    "        assert value >= 0, 'llr_max cannot be negative.'\n",
    "        self._llr_max = value\n",
    "\n",
    "    @property\n",
    "    def temperature(self):\n",
    "        \"\"\"Temperature for Gumble-softmax trick.\"\"\"\n",
    "        return self._temperature.item()\n",
    "\n",
    "    @temperature.setter\n",
    "    def temperature(self, value):\n",
    "        \"\"\"Temperature for Gumble-softmax trick.\"\"\"\n",
    "        assert value >= 0, 'temperature cannot be negative.'\n",
    "        self._temperature = torch.tensor(value, dtype=torch.float32)\n",
    "    #########################\n",
    "    # Utility methods\n",
    "    #########################\n",
    "\n",
    "    def _check_inputs(self, x):\n",
    "        \"\"\"Check input x for consistency, i.e., verify\n",
    "        that all values are binary of bipolar values.\"\"\"\n",
    "        x = x.float()\n",
    "        if self.check_input:\n",
    "            if self.bipolar_input:\n",
    "                assert torch.all(torch.logical_or(x == -1, x == 1)), \"Input must be bipolar {-1, 1}.\"\n",
    "            else:\n",
    "                assert torch.all(torch.logical_or(x == 0, x == 1)), \"Input must be binary {0, 1}.\"\n",
    "            # input datatype consistency should be only evaluated once\n",
    "            self.check_input = False\n",
    "\n",
    "    # 使用方法\n",
    "    @staticmethod\n",
    "    def custom_xor(a, b):\n",
    "        return CustomOperations.CustomXOR.apply(a, b)       \n",
    "\n",
    "    @staticmethod\n",
    "    def ste_binarizer(self, x):\n",
    "        \"\"\"Straight through binarizer to quantize bits to int values.\"\"\"\n",
    "        return CustomOperations.STEBinarizer.apply(x)\n",
    "\n",
    "    def _sample_errors(self, pb, shape):\n",
    "        \"\"\"Samples binary error vector with given error probability e.\n",
    "        This function is based on the Gumble-softmax \"trick\" to keep the\n",
    "        sampling differentiable.\"\"\"\n",
    "\n",
    "        u1 = torch.rand(shape)\n",
    "        u2 = torch.rand(shape)\n",
    "        u = torch.stack((u1, u2), dim=-1)\n",
    "\n",
    "        # sample Gumble distribution\n",
    "        q = -torch.log(-torch.log(u + self.eps) + self.eps)\n",
    "        p = torch.stack((pb, 1 - pb), dim=-1)\n",
    "        p = p.unsqueeze(0).expand(q.shape)\n",
    "        a = (torch.log(p + self.eps) + q) / self.temperature\n",
    "\n",
    "        # apply softmax\n",
    "        e_cat = F.softmax(a, dim=-1)\n",
    "\n",
    "        # binarize final values via straight-through estimator\n",
    "        return self._ste_binarizer(e_cat[..., 0])  # only take the first class\n",
    "    \n",
    "    #########################\n",
    "    # Keras layer functions\n",
    "    #########################\n",
    "\n",
    "    # 这段代码定义了一个 build 方法了，用于验证输入的形状是否正确\n",
    "    # 它主要检查第二个输入（错误概率 pb）的形状，确保其最后一维的长度为 2\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        \"\"\"Verify correct input shapes\"\"\"\n",
    "\n",
    "        pb_shapes = input_shapes[1]\n",
    "        # allow tuple of scalars as alternative input\n",
    "        if isinstance(pb_shapes, (tuple, list)):\n",
    "            if not len(pb_shapes) == 2:\n",
    "                raise ValueError(\"Last dim of pb must be of length 2.\")\n",
    "        else:\n",
    "            if len(pb_shapes) > 0:\n",
    "                if not pb_shapes[-1] == 2:\n",
    "                    raise ValueError(\"Last dim of pb must be of length 2.\")\n",
    "            else:\n",
    "                raise ValueError(\"Last dim of pb must be of length 2.\")\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Apply discrete binary memoryless channel to inputs.\"\"\"\n",
    "\n",
    "        x, pb = inputs\n",
    "\n",
    "        # allow pb to be a tuple of two scalars\n",
    "        if isinstance(pb, (tuple, list)):\n",
    "            pb0 = pb[0]\n",
    "            pb1 = pb[1]\n",
    "        else:\n",
    "            pb0 = pb[...,0]\n",
    "            pb1 = pb[...,1]\n",
    "        \n",
    "        # 假设pb0和pb1是PyTorch张量\n",
    "        pb0 = pb0.float()  # 确保pb0是浮点数\n",
    "        pb1 = pb1.float()  # 确保pb1是浮点数\n",
    "        pb0 = torch.clamp(pb0, 0., 1.)  # 将pb0的值限制在0和1之间\n",
    "        pb1 = torch.clamp(pb1, 0., 1.)  # 将pb1的值限制在0和1之间\n",
    "\n",
    "        # check x for consistency (binary, bipolar)\n",
    "        self._check_inputs(x)\n",
    "\n",
    "        e0 = self._sample_errors(pb0,x.shape)\n",
    "        e1 = self._sample_errors(pb1, x.shape)\n",
    "\n",
    "        if self._bipolar_input:\n",
    "            neutral_element = torch.tensor(-1, dtype=x.dtype)\n",
    "        else:\n",
    "            neutral_element = torch.tensor(0, dtype=x.dtype)    \n",
    "\n",
    "        # mask e0 and e1 with input such that e0 only applies where x==0    \n",
    "        e = torch.where(x == neutral_element, e0, e1)\n",
    "        e = e.to(dtype=x.dtype)\n",
    "\n",
    "        if self._bipolar_input:\n",
    "            # flip signs for bipolar case\n",
    "            y = x * (-2*e + 1)\n",
    "        else:\n",
    "            # XOR for binary case\n",
    "            y = self.custom_xor(x, e)\n",
    "\n",
    "        # if LLRs should be returned\n",
    "        if self._return_llrs:\n",
    "            if not self._bipolar_input:\n",
    "                y = 2 * y - 1  # transform to bipolar\n",
    "            # Remark: Sionna uses the logit definition log[p(x=1)/p(x=0)]\n",
    "            # 计算LLRs的组成部分\n",
    "            y0 = -(torch.log(pb1 + self._eps) - torch.log(1 - pb0 - self._eps))\n",
    "            y1 = (torch.log(1 - pb1 - self._eps) - torch.log(pb0 + self._eps))\n",
    "\n",
    "            # multiply by y to keep gradient\n",
    "            # 使用torch.where实现条件选择\n",
    "            y = torch.where(y == 1, y1, y0).to(dtype=y.dtype) * y\n",
    "\n",
    "            # and clip output llrs\n",
    "            # 将LLR的值限制在范围内\n",
    "            y = torch.clamp(y, min=-self._llr_max, max=self._llr_max)        \n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"这段代码定义了一个PyTorch模块`CustomOperations`，其中包含几个用于自定义操作的类和方法。以下是中文表述：\n",
    "\n",
    "### 类 `CustomOperations`：\n",
    "- 包含几个用于执行特定数学操作的内部类，包括`CustomXOR`和`STEBinarizer`，以及一个用于采样二元错误向量的模块`SampleErrors`。\n",
    "\n",
    "#### 类 `CustomXOR`（继承自`torch.autograd.Function`）：\n",
    "- 用于执行自定义的异或操作，支持整数和浮点数数据类型。\n",
    "- `forward`方法：执行异或操作，如果是整数类型则使用模2运算，否则使用绝对差值。\n",
    "- `backward`方法：在反向传播中，将梯度直接传回给输入。\n",
    "\n",
    "#### 类 `STEBinarizer`（继承自`torch.autograd.Function`）：\n",
    "- 用于实现Straight-Through Estimator Binarizer，即直通估计器二值化。\n",
    "- `forward`方法：在前向传播中，使用`torch.where`实现阈值化操作，将小于0.5的值设为0，否则设为1。\n",
    "- `backward`方法：在反向传播中，将梯度直接传回（直通估计器）。\n",
    "\n",
    "#### 类 `SampleErrors`（继承自`torch.nn.Module`）：\n",
    "- 用于根据给定的错误概率`pb`和形状`shape`采样二元错误向量。\n",
    "- 在初始化中接受一个小的正则化项`eps`和温度参数`temperature`。\n",
    "- `forward`方法：首先生成Gumbel分布的样本，然后通过softmax函数和直通估计器二值化来模拟二元错误。\n",
    "\n",
    "#### 类 `BinaryMemorylessChannel`（继承自`torch.nn.Module`）：\n",
    "- 用于模拟离散的二元对称信道，可以随机翻转比特位，翻转概率为`p_b`。\n",
    "- 接受是否返回LLRs的标志`return_llrs`，是否接受双极输入的标志`bipolar_input`，LLRs的最大值`llr_max`，数据类型`dtype`等参数。\n",
    "- 包含属性和方法用于设置和获取`llr_max`和`temperature`。\n",
    "- `_check_inputs`方法：检查输入`x`是否为二元或双极值。\n",
    "- `custom_xor`静态方法：调用`CustomXOR.apply`执行异或操作。\n",
    "- `ste_binarizer`静态方法：调用`STEBinarizer.apply`实现二值化。\n",
    "- `_sample_errors`方法：根据Gumbel-Softmax技巧采样二元错误向量。\n",
    "- `build`方法：验证输入形状是否正确。\n",
    "- `call`方法：应用离散二元对称信道到输入。\n",
    "\n",
    "在`call`方法中，根据是否返回LLRs、是否双极输入，以及给定的翻转概率`pb`，执行不同的操作来生成输出。如果需要返回LLRs，将输出转换为LLRs，并通过`torch.clamp`方法限制其值在`[-llr_max, llr_max]`范围内。\n",
    "\n",
    "整体上，这段代码提供了一个灵活的框架，用于在PyTorch中实现和使用自定义的神经网络操作，特别是那些涉及二值化和异或操作的场景。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关于STE\n",
    "[pytorch实现简单的straight-through estimator(STE)](https://segmentfault.com/a/1190000020993594)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在深度学习中一般我们学习的参数都是连续的，因为这样在反向传播的时候才可以对梯度进行更新。但是有的时候我们也会遇到参数是离>散的情况，这样就没有办法进行反向传播了，比如二值神经网络。本文中讲解了如何用`pytorch`对二值化的参数进行梯度更新的`straight-through estimator`算法。\n",
    "### Question\n",
    "`STE`核心的思想就是我们的参数初始化的时候就是`float`这样的连续值，当我们`forward`的时候就将原来的连续的参数映射到{-1,, 1}带入到网络进行计算，这样就可以计算网络的输出。然后`backward`的时候直接对原来`float`的参数进行更新，而不是对二值化的参数更新。这样可以完成对整个网络的更新了。\n",
    "首先我们对上面问题进行一下数学的讲解。\n",
    "\n",
    "- 我们希望参数的范围是$r \\in \\mathbb{R}$\n",
    "- 我们可以得到二值化的参数 $q = Sign(r)$, $Sign$函数可以参考`torch.sign`函数, 可以理解为取符号函数\n",
    "- `backward`的过程中对$q$求梯度可得 $\\frac{\\partial loss}{\\partial q}$\n",
    "- 对于$\\frac{\\partial q}{\\partial r} = 0$, 所以可以得出 $\\frac{\\partial loss}{\\partial r} = 0$, 这样的话我们就无法完成对参>数的更新，因为每次`loss`对`r`梯度都是0\n",
    "- 所以`backward`的过程我们需要修改$\\frac{\\partial q}{\\partial r}$这部分才可以使梯度继续更新下去，所以对$\\frac{\\partial loss}{\\partial r}$进行如下修改: $\\frac{\\partial q}{\\partial r} = \\frac{\\partial loss}{\\partial q} * 1\\_{|r| \\leq 1}$, 其中\n",
    "$1\\_{|r| \\leq 1}$ 可以看作$Htanh(x) = Clip(x, -1, 1) = max(-1, min(1, x))$对$x$的求导过程, 也就是是说:\n",
    "$$ \\frac{\\partial loss}{\\partial r} =  \\frac{\\partial loss}{\\partial q} \\frac{\\partial Htanh}{\\partial r}$$\n",
    "\n",
    "### Example\n",
    "#### torch.sign\n",
    "首先我们验证一下使用`torch.sign`会是参数的梯度基本上都是0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1455,  0.7444,  0.9377, -0.7715], requires_grad=True)\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(4, requires_grad = True)\n",
    "output = torch.sign(input)\n",
    "loss = output.mean()\n",
    "loss.backward()\n",
    "print(input)\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### demo\n",
    "我们需要重写sign这个函数，就好像写一个激活函数一样。先看一下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LBSign(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return torch.sign(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.clamp_(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们做一下测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1393, -0.2865,  1.7476, -0.3908], requires_grad=True)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500])\n"
     ]
    }
   ],
   "source": [
    "sign = LBSign.apply\n",
    "params = torch.randn(4, requires_grad = True)                                                                           \n",
    "output = sign(params)\n",
    "loss = output.mean()\n",
    "loss.backward()\n",
    "print(params)\n",
    "print(params.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mysionna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
