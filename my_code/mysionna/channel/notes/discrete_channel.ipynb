{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 11:31:32.898536: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-29 11:31:32.898571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-29 11:31:32.916038: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-29 11:31:32.954766: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-29 11:31:34.425510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from sionna.utils import expand_to_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BinaryMemorylessChannel类\n",
    "\n",
    "`BinaryMemorylessChannel`类是一个离散的二进制无记忆通道，具有（可能的）不对称比特翻转概率。\n",
    "\n",
    "输入的比特以概率 \\( p_\\text{b,0} \\) 和 \\( p_\\text{b,1} \\) 分别翻转。\n",
    "\n",
    "这个层支持二进制输入（\\( x \\in \\{0, 1\\} \\)）和双极性输入（\\( x \\in \\{-1, 1\\} \\)）。\n",
    "\n",
    "如果激活，该通道直接返回对数似然比（LLRs），定义如下：\n",
    "\n",
    "\\[\n",
    "\\ell =\n",
    "\\begin{cases}\n",
    "    \\operatorname{log} \\frac{p_{b,1}}{1-p_{b,0}}, \\qquad \\text{如果} \\, y=0 \\\\\n",
    "    \\operatorname{log} \\frac{1-p_{b,1}}{p_{b,0}}, \\qquad \\text{如果} \\, y=1 \\\\\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "错误概率 \\( p_\\text{b}\\) 可以是标量或张量（可广播到输入的形状）。这允许每个位位置有不同的擦除概率。在任何情况下，它的最后一个维度的长度必须为2，并被解释为 \\( p_\\text{b,0} \\) 和 \\( p_\\text{b,1} \\)。\n",
    "\n",
    "这个类继承自Keras的`Layer`类，可以在Keras模型中用作层。\n",
    "\n",
    "## 参数\n",
    "\n",
    "- `return_llrs`：布尔类型，默认值为 `False`。如果设置为 `True`，则该层返回基于 `pb` 的对数似然比（LLRs）而不是二进制值。\n",
    "\n",
    "- `bipolar_input`：布尔类型，默认值为 `False`。如果设置为 `True`，则期望的输入是 \\( \\{-1,1\\} \\) 而不是 \\( \\{0,1\\} \\)。\n",
    "\n",
    "- `llr_max`：`tf.float`类型，默认值为100。定义LLRs的裁剪值。\n",
    "\n",
    "- `dtype`：`tf.DType`类型，定义内部计算和输出的类型。默认值为 `tf.float32`。\n",
    "\n",
    "## 输入\n",
    "\n",
    "- `(x, pb)`：\n",
    "  - `x`：形状为 [...,n] 的 `tf.float32` 类型的输入序列，包含二进制值 \\( \\{0,1\\} \\) 或 \\( \\{-1,1\\} \\)。\n",
    "  - `pb`：形状为 [...,2] 的 `tf.float32` 类型的错误概率。可以是两个标量的元组或任何可以广播到 `x` 形状的形状。它有一个额外的最后维度，解释为 \\( p_\\text{b,0} \\) 和 \\( p_\\text{b,1} \\)。\n",
    "\n",
    "## 输出\n",
    "\n",
    "- 形状为 [...,n] 的 `tf.float32` 类型的输出序列，长度与输入 `x` 相同。如果 `return_llrs` 为 `False`，则输出是三元的，其中 `-1` 和 `0` 分别表示二进制和双极性输入的擦除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryMemorylessChannel(Layer):\n",
    "    def __init__(self, return_llrs=False, bipolar_input=False, llr_max=100.,dtype=tf.float32, **kwargs):\n",
    "\n",
    "        super().__init__(dtype=dtype,**kwargs)\n",
    "\n",
    "        assert isinstance(return_llrs, bool), \"return_llrs must be bool.\"\n",
    "        self._return_llrs = return_llrs\n",
    "\n",
    "        assert isinstance(bipolar_input, bool), \"bipolar_input must be bool.\"\n",
    "        self._bipolar_input = bipolar_input\n",
    "\n",
    "        assert llr_max>=0., \"llr_max must be a positive scalar value.\"\n",
    "        self._llr_max = tf.cast(llr_max, dtype=self.dtype)\n",
    "\n",
    "        if self._return_llrs:\n",
    "            assert dtype in (tf.float16, tf.float32, tf.float64),\\\n",
    "                \"LLR outputs require non-integer dtypes.\"\n",
    "        else:\n",
    "            if self._bipolar_input:\n",
    "                assert dtype in (tf.float16, tf.float32, tf.float64,\n",
    "                    tf.int8, tf.int16, tf.int32, tf.int64),\\\n",
    "                    \"Only, signed dtypes are supported for bipolar inputs.\"\n",
    "            else:\n",
    "                assert dtype in (tf.float16, tf.float32, tf.float64,\n",
    "                    tf.uint8, tf.uint16, tf.uint32, tf.uint64,\n",
    "                    tf.int8, tf.int16, tf.int32, tf.int64),\\\n",
    "                    \"Only, real-valued dtypes are supported.\"\n",
    "\n",
    "        self._check_input = True # check input for consistency (i.e., binary)\n",
    "\n",
    "        self._eps = 1e-9 # small additional term for numerical stability\n",
    "        self._temperature = tf.constant(0.1, tf.float32) # for Gumble-softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个`BinaryMemorylessChannel`类的构造函数中，有多个步骤来初始化和验证输入参数。下面是对每一行代码的详细解释：\n",
    "\n",
    "```python\n",
    "def __init__(self, return_llrs=False, bipolar_input=False, llr_max=100., dtype=tf.float32, **kwargs):\n",
    "```\n",
    "\n",
    "- 这是类的构造函数（初始化方法），用于在创建类的实例时设置初始值。\n",
    "- `return_llrs`: 是否返回对数似然比 (LLRs)，默认值为`False`。\n",
    "- `bipolar_input`: 输入是否为双极性的（{-1, 1}），默认值为`False`。\n",
    "- `llr_max`: LLRs的最大剪辑值，默认值为`100`。\n",
    "- `dtype`: 数据类型，默认值为`tf.float32`。\n",
    "- `**kwargs`: 其他可选参数。\n",
    "\n",
    "```python\n",
    "super().__init__(dtype=dtype, **kwargs)\n",
    "```\n",
    "\n",
    "- 调用父类`Layer`的初始化方法，并传递数据类型和其他参数。\n",
    "\n",
    "```python\n",
    "assert isinstance(return_llrs, bool), \"return_llrs must be bool.\"\n",
    "self._return_llrs = return_llrs\n",
    "```\n",
    "\n",
    "- 检查`return_llrs`是否为布尔值，如果不是，抛出一个断言错误。\n",
    "- 将`return_llrs`保存为实例变量`self._return_llrs`。\n",
    "\n",
    "```python\n",
    "assert isinstance(bipolar_input, bool), \"bipolar_input must be bool.\"\n",
    "self._bipolar_input = bipolar_input\n",
    "```\n",
    "\n",
    "- 检查`bipolar_input`是否为布尔值，如果不是，抛出一个断言错误。\n",
    "- 将`bipolar_input`保存为实例变量`self._bipolar_input`。\n",
    "\n",
    "```python\n",
    "assert llr_max >= 0., \"llr_max must be a positive scalar value.\"\n",
    "self._llr_max = tf.cast(llr_max, dtype=self.dtype)\n",
    "```\n",
    "\n",
    "- 检查`llr_max`是否为非负值，如果不是，抛出一个断言错误。\n",
    "- 将`llr_max`转换为指定的数据类型并保存为实例变量`self._llr_max`。\n",
    "\n",
    "```python\n",
    "if self._return_llrs:\n",
    "    assert dtype in (tf.float16, tf.float32, tf.float64), \"LLR outputs require non-integer dtypes.\"\n",
    "```\n",
    "\n",
    "- 如果返回LLRs，则检查数据类型是否为`tf.float16`、`tf.float32`或`tf.float64`中的一种。如果不是，抛出一个断言错误。\n",
    "\n",
    "```python\n",
    "else:\n",
    "    if self._bipolar_input:\n",
    "        assert dtype in (tf.float16, tf.float32, tf.float64, tf.int8, tf.int16, tf.int32, tf.int64),\\\n",
    "            \"Only, signed dtypes are supported for bipolar inputs.\"\n",
    "```\n",
    "\n",
    "- 如果不返回LLRs，且输入为双极性，则检查数据类型是否为`tf.float16`、`tf.float32`、`tf.float64`、`tf.int8`、`tf.int16`、`tf.int32`或`tf.int64`中的一种。如果不是，抛出一个断言错误。\n",
    "\n",
    "```python\n",
    "    else:\n",
    "        assert dtype in (tf.float16, tf.float32, tf.float64, tf.uint8, tf.uint16, tf.uint32, tf.uint64, tf.int8, tf.int16, tf.int32, tf.int64),\\\n",
    "            \"Only, real-valued dtypes are supported.\"\n",
    "```\n",
    "\n",
    "- 如果不返回LLRs，且输入不是双极性，则检查数据类型是否为`tf.float16`、`tf.float32`、`tf.float64`、`tf.uint8`、`tf.uint16`、`tf.uint32`、`tf.uint64`、`tf.int8`、`tf.int16`、`tf.int32`或`tf.int64`中的一种。如果不是，抛出一个断言错误。\n",
    "\n",
    "```python\n",
    "self._check_input = True # check input for consistency (i.e., binary)\n",
    "```\n",
    "\n",
    "- 设置一个实例变量`self._check_input`为`True`，用于在后续方法中检查输入的一致性（即是否为二进制）。\n",
    "\n",
    "```python\n",
    "self._eps = 1e-9 # small additional term for numerical stability\n",
    "```\n",
    "\n",
    "- 设置一个实例变量`self._eps`为`1e-9`，用于数值稳定性的小附加项。\n",
    "\n",
    "```python\n",
    "self._temperature = tf.constant(0.1, tf.float32) # for Gumble-softmax\n",
    "```\n",
    "\n",
    "- 设置一个实例变量`self._temperature`为常量`0.1`，用于Gumbel-Softmax。\n",
    "\n",
    "### 详细解释\n",
    "\n",
    "通过这些初始化步骤，`BinaryMemorylessChannel`类的构造函数确保了所有输入参数都是有效的，并根据这些参数设置了一些实例变量，用于后续的计算和操作。这些检查和设置有助于提高代码的健壮性和可维护性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 详细解释 `Gumbel-Softmax` 机制\n",
    "\n",
    "`Gumbel-Softmax` 是一种用于从离散分布中采样的近似方法，特别是在需要梯度信息的情况下。这在机器学习和深度学习中非常有用，例如在生成模型或离散动作空间的强化学习中。\n",
    "\n",
    "#### Gumbel 分布\n",
    "\n",
    "首先，`Gumbel-Softmax` 的基础是 Gumbel 分布。Gumbel 分布是一种极值分布，通常用于建模最大值或最小值的分布。其概率密度函数 (PDF) 为：\n",
    "\n",
    "$$\n",
    "f(x) = e^{-(x + e^{-x})}\n",
    "$$\n",
    "\n",
    "Gumbel 分布的一个重要性质是其用于最大值采样时的重参数化技巧。\n",
    "\n",
    "#### Gumbel-Max 采样\n",
    "\n",
    "给定一个类别分布 \\(\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_k)\\)，我们可以使用 Gumbel-Max 采样方法来从中采样一个类别：\n",
    "\n",
    "$$\n",
    "\\text{sample} = \\arg\\max_i \\left( \\log(\\pi_i) + g_i \\right)\n",
    "$$\n",
    "\n",
    "其中 $(g_i)$是从 Gumbel(0,1) 分布中采样的随机变量。\n",
    "\n",
    "#### Gumbel-Softmax\n",
    "\n",
    "然而，Gumbel-Max 采样是离散的，不适合梯度优化。因此，Gumbel-Softmax 引入了一个温度参数 $(\\tau)$，并将采样过程近似为：\n",
    "\n",
    "$$\n",
    "y_i = \\frac{\\exp\\left((\\log(\\pi_i) + g_i) / \\tau \\right)}{\\sum_{j=1}^k \\exp\\left((\\log(\\pi_j) + g_j) / \\tau \\right)}\n",
    "$$\n",
    "\n",
    "当 $(\\tau \\to 0)$ 时，Gumbel-Softmax 分布趋近于离散分布。当 $(\\tau \\to \\infty)$ 时，分布变得更加均匀。通过调整 $(\\tau)$，我们可以控制采样的离散程度。\n",
    "\n",
    "### `Gumbel-Softmax` 在 `BinaryMemorylessChannel` 中的应用\n",
    "\n",
    "在 `BinaryMemorylessChannel` 类的初始化过程中，设置 `self._temperature = tf.constant(0.1, tf.float32)` 是为了在某些计算中使用 `Gumbel-Softmax` 机制。具体来说，`Gumbel-Softmax` 可以用于模拟离散的二元信道，同时保持对梯度的支持。这对于神经网络的训练非常重要，因为它允许使用反向传播来优化参数。\n",
    "\n",
    "### 总结\n",
    "\n",
    "通过上述初始化和检查步骤，`BinaryMemorylessChannel` 类确保输入参数有效，并设置实例变量以支持后续的计算和操作。其中，`Gumbel-Softmax` 机制的引入，使得在处理离散信道采样时，能够有效地进行梯度优化，从而提高模型的性能和训练效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mysionna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
