{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-13 17:42:18.152895: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-13 17:42:18.152928: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-13 17:42:18.154659: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-13 17:42:18.163402: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-13 17:42:19.356527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "from my_code.mysionna.channel.torch_version.utils import expand_to_rank\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sionna.constants import GLOBAL_SEED_NUMBER \n",
    "\n",
    "class CustomOperations:\n",
    "    \n",
    "    class CustomXOR(Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, a, b):\n",
    "            ctx.save_for_backward(a, b)\n",
    "            if a.dtype in (torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64):\n",
    "                z = (a + b) % 2\n",
    "            else:\n",
    "                z = torch.abs(a - b)\n",
    "            return z\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            return grad_output, grad_output\n",
    "    # STEBinarizer（Straight-Through Estimator Binarizer）是一种在神经网络中用于处理二值化操作的技术。\n",
    "    # STE代表Straight-Through Estimator，它是一种用于在反向传播中处理不可微操作的技术。\n",
    "    class STEBinarizer(Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, x):\n",
    "            ctx.save_for_backward(x)\n",
    "            z = torch.where(x < 0.5, torch.tensor(0.0, device=x.device), torch.tensor(1.0, device=x.device))\n",
    "            return z\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            return grad_output\n",
    "    \n",
    "    class SampleErrors(torch.nn.Module):\n",
    "        def __init__(self, eps=1e-10, temperature=1.0):\n",
    "            super().__init__()\n",
    "            self._eps = eps\n",
    "            self._temperature = temperature\n",
    "\n",
    "        def forward(self, pb, shape):\n",
    "#            torch.manual_seed(2023)\n",
    "#            torch.cuda.manual_seed(2023)\n",
    "#            u1 = torch.rand(shape, dtype=torch.float32)\n",
    "#            u2 = torch.rand(shape, dtype=torch.float32)\n",
    "            tf.random.set_seed(GLOBAL_SEED_NUMBER)\n",
    "            u1_tf=tf.random.uniform(shape, minval=0, maxval=1, dtype=tf.float32)\n",
    "            u2_tf=tf.random.uniform(shape, minval=0, maxval=1, dtype=tf.float32)\n",
    "            u1_np=u1_tf.numpy()\n",
    "            u2_np=u2_tf.numpy()\n",
    "            u1=torch.from_numpy(u1_np)\n",
    "            u2=torch.from_numpy(u2_np)\n",
    "\n",
    "            u = torch.stack((u1, u2), dim=-1)\n",
    "\n",
    "            # 采样Gumbel分布\n",
    "            q = -torch.log(-torch.log(u + self._eps) + self._eps)\n",
    "            p = torch.stack((pb, 1 - pb), dim=-1).unsqueeze(1).expand(shape[0], shape[1], 2)\n",
    "            a = (torch.log(p + self._eps) + q) / self._temperature\n",
    "\n",
    "            # 应用softmax\n",
    "            e_cat = F.softmax(a, dim=-1)\n",
    "\n",
    "            # 通过直通估计器对最终值进行二值化\n",
    "            return CustomOperations.STEBinarizer.apply(e_cat[..., 0])\n",
    "\n",
    "class BinaryMemorylessChannel(nn.Module):\n",
    "    def __init__(self, return_llrs=False, bipolar_input=False, llr_max=100., dtype=torch.float32, **kwargs):\n",
    "        super(BinaryMemorylessChannel,self).__init__(**kwargs)\n",
    "\n",
    "        assert isinstance(return_llrs, bool), \"return_llrs must be bool.\"\n",
    "        self._return_llrs = return_llrs\n",
    "\n",
    "        assert isinstance(bipolar_input, bool), \"bipolar_input must be bool.\"\n",
    "        self._bipolar_input = bipolar_input\n",
    "\n",
    "        assert llr_max >= 0., \"llr_max must be a positive scalar value.\"\n",
    "        self.llr_max = llr_max\n",
    "        self.dtype = dtype\n",
    "\n",
    "        if self._return_llrs:\n",
    "            assert dtype in (torch.float16, torch.float32, torch.float64), \\\n",
    "                \"LLR outputs require non-integer dtypes.\"\n",
    "        else:\n",
    "            if self._bipolar_input:\n",
    "                assert dtype in (torch.float16, torch.float32, torch.float64,\n",
    "                                 torch.int8, torch.int16, torch.int32, torch.int64), \\\n",
    "                    \"Only signed dtypes are supported for bipolar inputs.\"\n",
    "            else:\n",
    "                assert dtype in (torch.float16, torch.float32, torch.float64,\n",
    "                                 torch.uint8, torch.int16, torch.int32, torch.int64,\n",
    "                                 torch.int8, torch.int16, torch.int32, torch.int64), \\\n",
    "                    \"Only real-valued dtypes are supported.\"\n",
    "\n",
    "        self.check_input = True  # check input for consistency (i.e., binary)\n",
    "\n",
    "        self._eps = 1e-9  # small additional term for numerical stability\n",
    "        self.temperature = torch.tensor(0.1, dtype=torch.float32)  # for Gumble-softmax\n",
    "\n",
    "    @property\n",
    "    def llr_max(self):\n",
    "        \"\"\"Maximum value used for LLR calculations.\"\"\"\n",
    "        return self._llr_max\n",
    "\n",
    "    @llr_max.setter\n",
    "    def llr_max(self, value):\n",
    "        \"\"\"Maximum value used for LLR calculations.\"\"\"\n",
    "        assert value >= 0, 'llr_max cannot be negative.'\n",
    "        self._llr_max = value\n",
    "\n",
    "    @property\n",
    "    def temperature(self):\n",
    "        \"\"\"Temperature for Gumble-softmax trick.\"\"\"\n",
    "        return self._temperature.item()\n",
    "\n",
    "    @temperature.setter\n",
    "    def temperature(self, value):\n",
    "        \"\"\"Temperature for Gumble-softmax trick.\"\"\"\n",
    "        assert value >= 0, 'temperature cannot be negative.'\n",
    "        self._temperature = torch.tensor(value, dtype=torch.float32)\n",
    "    #########################\n",
    "    # Utility methods\n",
    "    #########################\n",
    "\n",
    "    def _check_inputs(self, x):\n",
    "        \"\"\"Check input x for consistency, i.e., verify\n",
    "        that all values are binary of bipolar values.\"\"\"\n",
    "        x = x.float()\n",
    "        if self.check_input:\n",
    "            if self._bipolar_input:\n",
    "                assert torch.all(torch.logical_or(x == -1, x == 1)), \"Input must be bipolar {-1, 1}.\"\n",
    "            else:\n",
    "                assert torch.all(torch.logical_or(x == 0, x == 1)), \"Input must be binary {0, 1}.\"\n",
    "            # input datatype consistency should be only evaluated once\n",
    "            self.check_input = False\n",
    "\n",
    "    # 使用方法\n",
    "    @staticmethod\n",
    "    def custom_xor(a, b):\n",
    "        return CustomOperations.CustomXOR.apply(a, b)       \n",
    "\n",
    "    @staticmethod\n",
    "    def _ste_binarizer(x):\n",
    "        \"\"\"Straight through binarizer to quantize bits to int values.\"\"\"\n",
    "        return CustomOperations.STEBinarizer.apply(x)\n",
    "\n",
    "    def _sample_errors(self, pb, shape):\n",
    "        \"\"\"Samples binary error vector with given error probability e.\n",
    "        This function is based on the Gumble-softmax \"trick\" to keep the\n",
    "        sampling differentiable.\"\"\"\n",
    "#        torch.manual_seed(2023)\n",
    "#        torch.cuda.manual_seed(2023)\n",
    "#        u1 = torch.rand(shape)\n",
    "#        u2 = torch.rand(shape)\n",
    "        \n",
    "        tf.random.set_seed(GLOBAL_SEED_NUMBER)\n",
    "        u1_tf=tf.random.uniform(shape, minval=0, maxval=1, dtype=tf.float32)\n",
    "        u2_tf=tf.random.uniform(shape, minval=0, maxval=1, dtype=tf.float32)\n",
    "        u1_np=u1_tf.numpy()\n",
    "        u2_np=u2_tf.numpy()\n",
    "        u1=torch.from_numpy(u1_np)\n",
    "        u2=torch.from_numpy(u2_np)\n",
    "\n",
    "        u = torch.stack((u1, u2), dim=-1)\n",
    "\n",
    "        # sample Gumble distribution\n",
    "        q = -torch.log(-torch.log(u + self._eps) + self._eps)\n",
    "        p = torch.stack((pb, 1 - pb), dim=-1)\n",
    "        p = p.unsqueeze(0).expand(q.shape)\n",
    "        a = (torch.log(p + self._eps) + q) / self.temperature\n",
    "\n",
    "        # apply softmax\n",
    "        e_cat = F.softmax(a, dim=-1)\n",
    "\n",
    "        # binarize final values via straight-through estimator\n",
    "        return self._ste_binarizer(e_cat[..., 0])  # only take the first class\n",
    "    \n",
    "    #########################\n",
    "    # Keras layer functions\n",
    "    #########################\n",
    "\n",
    "    # 这段代码定义了一个 build 方法了，用于验证输入的形状是否正确\n",
    "    # 它主要检查第二个输入（错误概率 pb）的形状，确保其最后一维的长度为 2\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        \"\"\"Verify correct input shapes\"\"\"\n",
    "\n",
    "        pb_shapes = input_shapes[1]\n",
    "        # allow tuple of scalars as alternative input\n",
    "        if isinstance(pb_shapes, (tuple, list)):\n",
    "            if not len(pb_shapes) == 2:\n",
    "                raise ValueError(\"Last dim of pb must be of length 2.\")\n",
    "        else:\n",
    "            if len(pb_shapes) > 0:\n",
    "                if not pb_shapes[-1] == 2:\n",
    "                    raise ValueError(\"Last dim of pb must be of length 2.\")\n",
    "            else:\n",
    "                raise ValueError(\"Last dim of pb must be of length 2.\")\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Apply discrete binary memoryless channel to inputs.\"\"\"\n",
    "\n",
    "        x, pb = inputs\n",
    "\n",
    "        # allow pb to be a tuple of two scalars\n",
    "        if isinstance(pb, (tuple, list)):\n",
    "            pb0 = pb[0]\n",
    "            pb1 = pb[1]\n",
    "        else:\n",
    "            pb0 = pb[...,0]\n",
    "            pb1 = pb[...,1]\n",
    "        \n",
    "        # 假设pb0和pb1是PyTorch张量\n",
    "        pb0 = pb0.float()  # 确保pb0是浮点数\n",
    "        pb1 = pb1.float()  # 确保pb1是浮点数\n",
    "        pb0 = torch.clamp(pb0, 0., 1.)  # 将pb0的值限制在0和1之间\n",
    "        pb1 = torch.clamp(pb1, 0., 1.)  # 将pb1的值限制在0和1之间\n",
    "\n",
    "        # check x for consistency (binary, bipolar)\n",
    "        self._check_inputs(x)\n",
    "\n",
    "        e0 = self._sample_errors(pb0,x.shape)\n",
    "        e1 = self._sample_errors(pb1, x.shape)\n",
    "\n",
    "        if self._bipolar_input:\n",
    "            neutral_element = torch.tensor(-1, dtype=x.dtype)\n",
    "        else:\n",
    "            neutral_element = torch.tensor(0, dtype=x.dtype)    \n",
    "\n",
    "        # mask e0 and e1 with input such that e0 only applies where x==0    \n",
    "        e = torch.where(x == neutral_element, e0, e1)\n",
    "        e = e.to(dtype=x.dtype)\n",
    "\n",
    "        if self._bipolar_input:\n",
    "            # flip signs for bipolar case\n",
    "            y = x * (-2*e + 1)\n",
    "        else:\n",
    "            # XOR for binary case\n",
    "            y = self.custom_xor(x, e)\n",
    "\n",
    "        # if LLRs should be returned\n",
    "        if self._return_llrs:\n",
    "            if not self._bipolar_input:\n",
    "                y = 2 * y - 1  # transform to bipolar\n",
    "            # Remark: Sionna uses the logit definition log[p(x=1)/p(x=0)]\n",
    "            # 计算LLRs的组成部分\n",
    "            y0 = -(torch.log(pb1 + self._eps) - torch.log(1 - pb0 - self._eps))\n",
    "            y1 = (torch.log(1 - pb1 - self._eps) - torch.log(pb0 + self._eps))\n",
    "\n",
    "            # multiply by y to keep gradient\n",
    "            # 使用torch.where实现条件选择\n",
    "            y = torch.where(y == 1, y1, y0).to(dtype=y.dtype) * y\n",
    "\n",
    "            # and clip output llrs\n",
    "            # 将LLR的值限制在范围内\n",
    "            y = torch.clamp(y, min=-self._llr_max, max=self._llr_max)        \n",
    "\n",
    "        return y\n",
    "\n",
    "class BinarySymmetricChannel(BinaryMemorylessChannel):\n",
    "    def __init__(self,return_llrs=False,bipolar_input=False,llr_max=100.,dtype=torch.float32,**kwargs):\n",
    "        #继承父类的__init__()\n",
    "        super().__init__(return_llrs=return_llrs,\n",
    "                         bipolar_input=bipolar_input,\n",
    "                         llr_max=llr_max,\n",
    "                         dtype=dtype,\n",
    "                         **kwargs)\n",
    "    #########################\n",
    "    # Keras layer functions\n",
    "    #########################\n",
    "\n",
    "    def build(self,input_shapes):\n",
    "        \"\"\"\"Verify correct input shapes\"\"\"\n",
    "        pass # nothing to verify here\n",
    "    def forward(self,inputs):\n",
    "        \"\"\"Apply discrete binary symmetric channel, i.e., randomly flip\n",
    "        bits with probability pb.\"\"\"\n",
    "        \"\"\"\"应用离散二进制对称信道,即以pb概率随机翻转位\"\"\"\n",
    "\n",
    "        x,pb = inputs\n",
    "\n",
    "        # the BSC is implemented by calling the DMC with symmetric pb\n",
    "        # BSC（二元对称信道）是通过使用对称的pb（比特翻转概率）调用DMC（离散记忆少信道）来实现的\n",
    "        # 这里的“对称的pb”意味着信道的翻转概率p对于输入0和1是相同的，即信道以相同的概率将输入0翻转为1，或将输入1翻转为0。\n",
    "\n",
    "        \"\"\"\"在二元对称信道(BSC)中,通常有两种状态:输入0被保持为0,或被翻转为1;输入1被保持为1,或被翻转为0。\n",
    "        如果翻转概率p对于两种输入都是相同的,那么信道就是对称的。\n",
    "\n",
    "        在实现上,可以构建一个离散记忆少信道(DMC),并设置其状态转移概率矩阵为对称的,以此来模拟BSC的行为。\n",
    "\n",
    "        在数学上,如果用p表示翻转概率,那么BSC的状态转移概率可以表示为:\n",
    "        从状态0(输入0)翻转到状态1的概率是p   从状态1(输入1)翻转到状态0的概率也是p  保持原始状态的概率是1-p\"\"\"\n",
    "        pb = pb.to(x.dtype)\n",
    "        pb = torch.stack((pb,pb), dim=-1)\n",
    "        y = super().forward((x,pb))\n",
    "\n",
    "        return y\n",
    "\n",
    "class BinaryZChannel(BinaryMemorylessChannel):\n",
    "    def __init__(self, return_llrs=False, bipolar_input=False,llr_max=100.,dtype=torch.float32, **kwargs):\n",
    "\n",
    "        super().__init__(return_llrs=return_llrs,\n",
    "                         bipolar_input=bipolar_input,\n",
    "                         llr_max=llr_max,\n",
    "                         dtype=dtype,\n",
    "                         **kwargs)\n",
    "    #########################\n",
    "    # Keras layer functions\n",
    "    #########################\n",
    "    def build(self, input_shapes):\n",
    "        \"\"\"Verify correct input shapes\"\"\"\n",
    "        pass # nothing to verify here\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Apply discrete binary symmetric channel, i.e., randomly flip\n",
    "        bits with probability pb.\"\"\"\n",
    "\n",
    "        x, pb = inputs\n",
    "\n",
    "        # the Z is implemented by calling the DMC with p(1|0)=0\n",
    "        pb = pb.to(x.dtype)\n",
    "        pb = torch.stack((torch.zeros_like(pb), pb), dim=-1)\n",
    "        y = super().forward((x, pb))\n",
    "\n",
    "        return y        \n",
    "  \n",
    "\n",
    "class BinaryErasureChannel(BinaryMemorylessChannel):\n",
    "    def __init__(self, return_llrs=False, bipolar_input=False, llr_max=100, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.return_llrs = return_llrs\n",
    "        self.bipolar_input = bipolar_input\n",
    "        self.llr_max = llr_max\n",
    "        self.dtype = dtype\n",
    "\n",
    "        assert dtype in (torch.float16, torch.float32, torch.float64,\n",
    "                         torch.int8, torch.int16, torch.int32, torch.int64), \\\n",
    "               \"Unsigned integers are currently not supported.\"\n",
    "    #########################\n",
    "    # Keras layer functions\n",
    "    #########################\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x,pb = inputs\n",
    "\n",
    "\n",
    "        # Example validation of input x\n",
    "        if not self.bipolar_input:\n",
    "            assert torch.all((x == 0) | (x == 1)), \"Input x must be binary (0 or 1).\"\n",
    "        else:\n",
    "            assert torch.all((x == -1) | (x == 1)), \"Input x must be bipolar (-1 or 1).\"\n",
    "\n",
    "        # Example validation of pb\n",
    "        # clip for numerical stability\n",
    "        pb = pb.float().clamp(0., 1.)\n",
    "\n",
    "        # sample erasure pattern\n",
    "        e = self._sample_errors(pb, x.size())\n",
    "\n",
    "        # if LLRs should be returned\n",
    "        # remark: the Sionna logit definition is llr = log[p(x=1)/p(x=0)]\n",
    "        if self.return_llrs:\n",
    "            if not self.bipolar_input:\n",
    "                x = 2 * x - 1\n",
    "            x = x.to(torch.float32) * self.llr_max  # calculate llrs\n",
    "\n",
    "            # erase positions by setting llrs to 0\n",
    "            y = torch.where(e == 1, torch.tensor(0, dtype=torch.float32), x)\n",
    "        else:\n",
    "            if self.bipolar_input:\n",
    "                erased_element = torch.tensor(0, dtype=x.dtype) \n",
    "            else:\n",
    "                erased_element=torch.tensor(-1, dtype=x.dtype)\n",
    "            y = torch.where(e == 0, x, erased_element)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据 x:\n",
      "tensor([[0., 1., 0., 1.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0.]])\n",
      "\n",
      "错误概率 pb:\n",
      "tensor([[0.1000, 0.9000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.2000, 0.8000],\n",
      "        [0.8000, 0.2000]])\n",
      "\n",
      "输出数据 y:\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3745/1948812531.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self._temperature = torch.tensor(value, dtype=torch.float32)\n",
      "2024-07-13 17:42:23.022631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1050 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# 测试 BinaryMemorylessChannel 用例\n",
    "def test_binary_memoryless_channel():\n",
    "    # 定义更复杂的输入数据\n",
    "    x = torch.tensor([[0, 1, 0, 1], [1, 0, 1, 0], [0, 0, 1, 1], [1, 1, 0, 0]], dtype=torch.float32)\n",
    "    pb = torch.tensor([[0.1, 0.9], [0.5, 0.5], [0.2, 0.8], [0.8, 0.2]], dtype=torch.float32)  # 各种不同的概率\n",
    "\n",
    "    # 创建 BinaryMemorylessChannel 实例\n",
    "    bmc = BinaryMemorylessChannel(return_llrs=True, bipolar_input=False, llr_max=100.)\n",
    "\n",
    "    # 执行前向传播\n",
    "    y = bmc((x, pb))\n",
    "\n",
    "    # 打印输入和输出\n",
    "    print(\"输入数据 x:\")\n",
    "    print(x)\n",
    "    print(\"\\n错误概率 pb:\")\n",
    "    print(pb)\n",
    "    print(\"\\n输出数据 y:\")\n",
    "    print(y)\n",
    "\n",
    "# 运行测试用例\n",
    "test_binary_memoryless_channel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据 x:\n",
      "tensor([[0., 1., 0., 1.],\n",
      "        [1., 0., 1., 0.]])\n",
      "\n",
      "翻转概率 pb:\n",
      "tensor(0.3700)\n",
      "\n",
      "输出数据 y:\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3745/1948812531.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self._temperature = torch.tensor(value, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# 测试 BinarySymmetricChannel 类\n",
    "# 定义输入数据\n",
    "x = torch.tensor([[0, 1, 0, 1], [1, 0, 1, 0]], dtype=torch.float32)\n",
    "pb = torch.tensor(0.37, dtype=torch.float32)  # 10% 概率翻转\n",
    "\n",
    "# 创建 BinarySymmetricChannel 实例\n",
    "bsc = BinarySymmetricChannel()\n",
    "\n",
    "# 执行前向传播\n",
    "y = bsc.forward((x, pb))\n",
    "\n",
    "# 打印输入和输出\n",
    "print(\"输入数据 x:\")\n",
    "print(x)\n",
    "print(\"\\n翻转概率 pb:\")\n",
    "print(pb)\n",
    "print(\"\\n输出数据 y:\")\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据 x:\n",
      "tensor([[0., 1., 0., 1.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0.]])\n",
      "\n",
      "翻转概率 pb:\n",
      "tensor([0.1000, 0.3000, 0.2000, 0.8000])\n",
      "\n",
      "输出数据 y:\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 1., 0., 0.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3745/1948812531.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self._temperature = torch.tensor(value, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# 测试 BinaryZChannel 类\n",
    "# 定义更复杂的输入数据\n",
    "x = torch.tensor([[0, 1, 0, 1], [1, 0, 1, 0], [0, 0, 1, 1], [1, 1, 0, 0]], dtype=torch.float32)\n",
    "pb = torch.tensor([0.1, 0.3, 0.2, 0.8], dtype=torch.float32)  # 各种不同的概率\n",
    "\n",
    "# 创建 BinaryZChannel 实例\n",
    "bzc = BinaryZChannel()\n",
    "\n",
    "# 执行前向传播\n",
    "y = bzc.forward((x, pb))\n",
    "\n",
    "# 打印输入和输出\n",
    "print(\"输入数据 x:\")\n",
    "print(x)\n",
    "print(\"\\n翻转概率 pb:\")\n",
    "print(pb)\n",
    "print(\"\\n输出数据 y:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据 input_data:\n",
      "[-1.  1. -1.  1.  1.]\n",
      "\n",
      "翻转概率 pb:\n",
      "0.48\n",
      "\n",
      "输出数据 output:\n",
      "[-1.  0. -1.  0.  1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3745/1948812531.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self._temperature = torch.tensor(value, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# 测试 BinaryErasureChannel 类\n",
    "# Usage example\n",
    "input_data = torch.tensor([-1, 1, -1, 1, 1], dtype=torch.float32)\n",
    "pb = torch.tensor(0.48)\n",
    "channel = BinaryErasureChannel(return_llrs=False, bipolar_input=True)\n",
    "\n",
    "output = channel((input_data, pb))\n",
    "\n",
    "# 打印输入和输出\n",
    "print(\"输入数据 input_data:\")\n",
    "print(input_data.numpy())\n",
    "print(\"\\n翻转概率 pb:\")\n",
    "print(pb.numpy())\n",
    "print(\"\\n输出数据 output:\")\n",
    "print(output.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BinaryMemorylessChannel类\n",
    "\n",
    "`BinaryMemorylessChannel`类是一个离散的二进制无记忆通道，具有（可能的）不对称比特翻转概率。\n",
    "\n",
    "输入的比特以概率 \\( p_\\text{b,0} \\) 和 \\( p_\\text{b,1} \\) 分别翻转。\n",
    "\n",
    "这个层支持二进制输入（\\( x \\in \\{0, 1\\} \\)）和双极性输入（\\( x \\in \\{-1, 1\\} \\)）。\n",
    "\n",
    "如果激活，该通道直接返回对数似然比（LLRs），定义如下：\n",
    "\n",
    "\\[\n",
    "\\ell =\n",
    "\\begin{cases}\n",
    "    \\operatorname{log} \\frac{p_{b,1}}{1-p_{b,0}}, \\qquad \\text{如果} \\, y=0 \\\\\n",
    "    \\operatorname{log} \\frac{1-p_{b,1}}{p_{b,0}}, \\qquad \\text{如果} \\, y=1 \\\\\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "错误概率 \\( p_\\text{b}\\) 可以是标量或张量（可广播到输入的形状）。这允许每个位位置有不同的擦除概率。在任何情况下，它的最后一个维度的长度必须为2，并被解释为 \\( p_\\text{b,0} \\) 和 \\( p_\\text{b,1} \\)。\n",
    "\n",
    "这个类继承自Keras的`Layer`类，可以在Keras模型中用作层。\n",
    "\n",
    "## 参数\n",
    "\n",
    "- `return_llrs`：布尔类型，默认值为 `False`。如果设置为 `True`，则该层返回基于 `pb` 的对数似然比（LLRs）而不是二进制值。\n",
    "\n",
    "- `bipolar_input`：布尔类型，默认值为 `False`。如果设置为 `True`，则期望的输入是 \\( \\{-1,1\\} \\) 而不是 \\( \\{0,1\\} \\)。\n",
    "\n",
    "- `llr_max`：`tf.float`类型，默认值为100。定义LLRs的裁剪值。\n",
    "\n",
    "- `dtype`：`tf.DType`类型，定义内部计算和输出的类型。默认值为 `tf.float32`。\n",
    "\n",
    "## 输入\n",
    "\n",
    "- `(x, pb)`：\n",
    "  - `x`：形状为 [...,n] 的 `tf.float32` 类型的输入序列，包含二进制值 \\( \\{0,1\\} \\) 或 \\( \\{-1,1\\} \\)。\n",
    "  - `pb`：形状为 [...,2] 的 `tf.float32` 类型的错误概率。可以是两个标量的元组或任何可以广播到 `x` 形状的形状。它有一个额外的最后维度，解释为 \\( p_\\text{b,0} \\) 和 \\( p_\\text{b,1} \\)。\n",
    "\n",
    "## 输出\n",
    "\n",
    "- 形状为 [...,n] 的 `tf.float32` 类型的输出序列，长度与输入 `x` 相同。如果 `return_llrs` 为 `False`，则输出是三元的，其中 `-1` 和 `0` 分别表示二进制和双极性输入的擦除。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 详细解释 `Gumbel-Softmax` 机制\n",
    "\n",
    "`Gumbel-Softmax` 是一种用于从离散分布中采样的近似方法，特别是在需要梯度信息的情况下。这在机器学习和深度学习中非常有用，例如在生成模型或离散动作空间的强化学习中。\n",
    "\n",
    "#### Gumbel 分布\n",
    "\n",
    "首先，`Gumbel-Softmax` 的基础是 Gumbel 分布。Gumbel 分布是一种极值分布，通常用于建模最大值或最小值的分布。其概率密度函数 (PDF) 为：\n",
    "\n",
    "$$\n",
    "f(x) = e^{-(x + e^{-x})}\n",
    "$$\n",
    "\n",
    "Gumbel 分布的一个重要性质是其用于最大值采样时的重参数化技巧。\n",
    "\n",
    "#### Gumbel-Max 采样\n",
    "\n",
    "给定一个类别分布 \\(\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_k)\\)，我们可以使用 Gumbel-Max 采样方法来从中采样一个类别：\n",
    "\n",
    "$$\n",
    "\\text{sample} = \\arg\\max_i \\left( \\log(\\pi_i) + g_i \\right)\n",
    "$$\n",
    "\n",
    "其中 $(g_i)$是从 Gumbel(0,1) 分布中采样的随机变量。\n",
    "\n",
    "#### Gumbel-Softmax\n",
    "\n",
    "然而，Gumbel-Max 采样是离散的，不适合梯度优化。因此，Gumbel-Softmax 引入了一个温度参数 $(\\tau)$，并将采样过程近似为：\n",
    "\n",
    "$$\n",
    "y_i = \\frac{\\exp\\left((\\log(\\pi_i) + g_i) / \\tau \\right)}{\\sum_{j=1}^k \\exp\\left((\\log(\\pi_j) + g_j) / \\tau \\right)}\n",
    "$$\n",
    "\n",
    "当 $(\\tau \\to 0)$ 时，Gumbel-Softmax 分布趋近于离散分布。当 $(\\tau \\to \\infty)$ 时，分布变得更加均匀。通过调整 $(\\tau)$，我们可以控制采样的离散程度。\n",
    "\n",
    "### `Gumbel-Softmax` 在 `BinaryMemorylessChannel` 中的应用\n",
    "\n",
    "在 `BinaryMemorylessChannel` 类的初始化过程中，设置 `self._temperature = tf.constant(0.1, tf.float32)` 是为了在某些计算中使用 `Gumbel-Softmax` 机制。具体来说，`Gumbel-Softmax` 可以用于模拟离散的二元信道，同时保持对梯度的支持。这对于神经网络的训练非常重要，因为它允许使用反向传播来优化参数。\n",
    "\n",
    "### 总结\n",
    "\n",
    "通过上述初始化和检查步骤，`BinaryMemorylessChannel` 类确保输入参数有效，并设置实例变量以支持后续的计算和操作。其中，`Gumbel-Softmax` 机制的引入，使得在处理离散信道采样时，能够有效地进行梯度优化，从而提高模型的性能和训练效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关于STE\n",
    "[pytorch实现简单的straight-through estimator(STE)](https://segmentfault.com/a/1190000020993594)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在深度学习中一般我们学习的参数都是连续的，因为这样在反向传播的时候才可以对梯度进行更新。但是有的时候我们也会遇到参数是离>散的情况，这样就没有办法进行反向传播了，比如二值神经网络。本文中讲解了如何用`pytorch`对二值化的参数进行梯度更新的`straight-through estimator`算法。\n",
    "### Question\n",
    "`STE`核心的思想就是我们的参数初始化的时候就是`float`这样的连续值，当我们`forward`的时候就将原来的连续的参数映射到{-1,, 1}带入到网络进行计算，这样就可以计算网络的输出。然后`backward`的时候直接对原来`float`的参数进行更新，而不是对二值化的参数更新。这样可以完成对整个网络的更新了。\n",
    "首先我们对上面问题进行一下数学的讲解。\n",
    "\n",
    "- 我们希望参数的范围是$r \\in \\mathbb{R}$\n",
    "- 我们可以得到二值化的参数 $q = Sign(r)$, $Sign$函数可以参考`torch.sign`函数, 可以理解为取符号函数\n",
    "- `backward`的过程中对$q$求梯度可得 $\\frac{\\partial loss}{\\partial q}$\n",
    "- 对于$\\frac{\\partial q}{\\partial r} = 0$, 所以可以得出 $\\frac{\\partial loss}{\\partial r} = 0$, 这样的话我们就无法完成对参>数的更新，因为每次`loss`对`r`梯度都是0\n",
    "- 所以`backward`的过程我们需要修改$\\frac{\\partial q}{\\partial r}$这部分才可以使梯度继续更新下去，所以对$\\frac{\\partial loss}{\\partial r}$进行如下修改: $\\frac{\\partial q}{\\partial r} = \\frac{\\partial loss}{\\partial q} * 1\\_{|r| \\leq 1}$, 其中\n",
    "$1\\_{|r| \\leq 1}$ 可以看作$Htanh(x) = Clip(x, -1, 1) = max(-1, min(1, x))$对$x$的求导过程, 也就是是说:\n",
    "$$ \\frac{\\partial loss}{\\partial r} =  \\frac{\\partial loss}{\\partial q} \\frac{\\partial Htanh}{\\partial r}$$\n",
    "\n",
    "### Example\n",
    "#### torch.sign\n",
    "首先我们验证一下使用`torch.sign`会是参数的梯度基本上都是0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0786, 0.3029, 0.5720, 2.0079], requires_grad=True)\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(4, requires_grad = True)\n",
    "output = torch.sign(input)\n",
    "loss = output.mean()\n",
    "loss.backward()\n",
    "print(input)\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### demo\n",
    "我们需要重写sign这个函数，就好像写一个激活函数一样。先看一下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LBSign(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return torch.sign(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.clamp_(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们做一下测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6866, 0.7378, 0.5175, 0.1240], requires_grad=True)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500])\n"
     ]
    }
   ],
   "source": [
    "sign = LBSign.apply\n",
    "params = torch.randn(4, requires_grad = True)                                                                           \n",
    "output = sign(params)\n",
    "loss = output.mean()\n",
    "loss.backward()\n",
    "print(params)\n",
    "print(params.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explain\n",
    "接下来我们对代码就行一下解释[pytorch文档链接](https://pytorch.org/docs/stable/autograd.html#function):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forward中的参数ctx是保存的上下文信息，input是输入\n",
    "- backward中的参数ctx是保存的上下文信息，grad_output可以理解成 $\\frac{\\partial loss}{\\partial q}$这一步的梯度信息，\n",
    "\n",
    "我们需要做的就是让$$grad_Output * \\frac{\\partial Htanh}{\\partial r} $$\n",
    "\n",
    "而不是让pytorch继续默认的$$ grad_Output * \\frac{\\partial q}{\\partial r} $$\n",
    "\n",
    "但是我们可以从上面的公式可以看出函数$Htanh$对$x$求导是1， 当$x \\in [-1, 1]$，所以程序就可以化简成保留原来的梯度就行了，然后裁剪到其他范围的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference\n",
    "[torch.autograd.Function](https://pytorch.org/docs/stable/autograd.html#function)\n",
    "\n",
    "[Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](https://arxiv.org/abs/1602.02830)\n",
    "\n",
    "[二值网络，围绕STE的那些事儿](https://zhuanlan.zhihu.com/p/72681647)\n",
    "\n",
    "[Custom binarization layer with straight through estimator gives error](https://discuss.pytorch.org/t/custom-binarization-layer-with-straight-through-estimator-gives-error/4539)\n",
    "\n",
    "[定义torch.autograd.Function的子类，自己定义某些操作，且定义反向求导函数](https://blog.csdn.net/tsq292978891/article/details/79364140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BinarySymmetricChannel\n",
    "这段代码定义了一个名为 `BinarySymmetricChannel` 的类，它继承自 `BinaryMemorylessChannel` 类，并实现了一个离散的二元对称信道（Binary Symmetric Channel, BSC）。以下是代码的主要组成部分和功能分析：\n",
    "\n",
    "### 类定义和初始化 (`__init__` 方法)\n",
    "- `BinarySymmetricChannel` 类接受几个参数：\n",
    "  - `return_llrs`: 布尔值，默认为 `False`。如果设置为 `True`，则信道返回对数似然比（LLRs）而不是基于 `pb` 的二元值。\n",
    "  - `bipolar_input`: 布尔值，默认为 `False`。如果设置为 `True`，则预期的输入为 {-1,1} 而不是 {0,1}。\n",
    "  - `llr_max`: 浮点型，默认为 100.0。定义了 LLRs 的裁剪值。\n",
    "  - `dtype`: 张量数据类型，默认为 `tf.float32`，用于内部计算和输出数据类型。\n",
    "- 调用父类 `BinaryMemorylessChannel` 的初始化方法，传递这些参数。\n",
    "\n",
    "### 构建 (`build` 方法)\n",
    "- `build` 方法用于验证输入形状是否正确。当前实现中，这个方法什么也不做（`pass`），表示没有特定的形状验证逻辑。\n",
    "\n",
    "### 调用 (`call` 方法)\n",
    "- `call` 方法是类的核心，实现了 BSC 的功能：\n",
    "  - 接收输入 `inputs`，它是一个包含 `x` 和 `pb` 的元组。`x` 是输入序列，`pb` 是比特翻转概率。\n",
    "  - 将 `pb` 转换为与 `x` 相同的数据类型，并将其堆叠为形状为 [-1, 2] 的张量，这表示 BSC 的翻转概率是对称的（即，翻转和不翻转的概率相同）。\n",
    "  - 调用父类的 `call` 方法，传入 `(x, pb)`，以应用二元对称信道。\n",
    "\n",
    "### 其他要点\n",
    "- 类注释提供了关于 BSC 的详细数学定义和使用场景的说明。\n",
    "- BSC 支持二元输入和双极输入。\n",
    "- 如果启用，信道可以直接返回 LLRs，这是在信息论中用于表示输入为 0 或 1 的对数概率比。\n",
    "- 比特翻转概率 `pb` 可以是标量或张量，允许每个比特位置具有不同的翻转概率。\n",
    "- 这个类继承自 Keras 的 `Layer` 类，因此可以作为 Keras 模型中的层使用。\n",
    "\n",
    "### 总结\n",
    "`BinarySymmetricChannel` 类是一个用于模拟二元对称信道的神经网络层，它可以作为 Keras 模型的一部分。它提供了灵活性，以处理不同的概率分布和输入类型，并可以返回原始的二元输出或 LLRs，这在某些信号处理和通信系统中非常有用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BinaryZChannel\n",
    "这段代码定义了一个名为 `BinaryZChannel` 的类，它继承自 `BinaryMemorylessChannel` 类，并实现了一个离散的二元 Z信道（Binary Z-Channel）。以下是代码的主要组成部分和功能分析：\n",
    "\n",
    "### 类定义和初始化 (`__init__` 方法)\n",
    "- `BinaryZChannel` 类接受几个参数：\n",
    "  - `return_llrs`: 布尔值，默认为 `False`。如果设置为 `True`，则层返回对数似然比（LLRs）而不是基于 `pb` 的二元值。\n",
    "  - `bipolar_input`: 布尔值，默认为 `False`。如果设为 `True`，则预期的输入为 {-1,1} 而不是 {0,1}。\n",
    "  - `llr_max`: 浮点型，默认为 100.0。定义了 LLRs 的裁剪值。\n",
    "  - `dtype`: 张量数据类型，默认为 `tf.float32`，用于内部计算和输出数据类型。\n",
    "- 调用父类 `BinaryMemorylessChannel` 的构造函数，传递这些参数。\n",
    "\n",
    "### 构建 (`build` 方法)\n",
    "- `build` 方法用于验证输入形状是否正确。当前实现中，这个方法什么也不做（`pass`），表示没有特定的形状验证逻辑。\n",
    "\n",
    "### 调用 (`call` 方法)\n",
    "- `call` 方法是类的核心，实现了 Z 信道的功能：\n",
    "  - 接收输入 `inputs`，它是一个包含 `x` 和 `pb` 的元组。`x` 是输入序列，`pb` 是错误概率。\n",
    "  - 将 `pb` 转换为与 `x` 相同的数据类型，并将其堆叠为形状为 [-1, 2] 的张量，这表示 Z 信道的错误模型，其中第一个元素（`0`）总是被正确接收，而第二个元素（`1`）以概率 `pb` 发生错误。\n",
    "  - 调用父类的 `call` 方法，传入 `(x, pb)`，以应用二元 Z 信道。\n",
    "\n",
    "### 其他要点\n",
    "- 类注释提供了关于 Z 信道的详细数学定义和使用场景的说明。\n",
    "- Z 信道只对第二个输入元素（即 `1`）发生错误，第一个元素（`0`）总是被正确接收。\n",
    "- 如果启用，信道可以直接返回 LLRs，这是在信息论中用于表示输入为 `0` 或 `1` 的对数概率比。\n",
    "- 错误概率 `pb` 可以是标量或张量，允许每个比特位置具有不同的错误概率。\n",
    "- 这个类继承自 Keras 的 `Layer` 类，因此可以作为 Keras 模型中的层使用。\n",
    "\n",
    "### 总结\n",
    "`BinaryZChannel` 类是一个用于模拟二元 Z 信道的神经网络层，它可以作为 Keras 模型的一部分。它提供了灵活性，以处理不同的概率分布和输入类型，并可以返回原始的二元输出或 LLRs，这在某些信号处理和通信系统中非常有用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z信道\n",
    "Z信道（Z-Channel）是一种理论上的通信信道模型，它具有一些特殊的错误模式。在Z信道中，只有当传输的信息是\"1\"时才会发生传输错误，而传输\"0\"总是被正确接收。这种信道模型通常用于信息论、编码理论和数字通信领域的教学和研究，以探讨错误检测和纠正算法的性能。\n",
    "\n",
    "### Z信道的特点：\n",
    "\n",
    "1. **\"0\"总是正确接收**：无论信道条件如何，传输的\"0\"总是能够被接收方正确识别。\n",
    "\n",
    "2. **\"1\"可能发生错误**：传输的\"1\"有一定的概率被错误地接收为\"0\"，这个概率通常用`p`表示。\n",
    "\n",
    "3. **错误概率对称**：在Z信道中，\"1\"被错误接收为\"0\"的概率与\"0\"被错误接收为\"1\"的概率相同，都是`p`。\n",
    "\n",
    "4. **对数似然比（LLR）**：在某些应用中，Z信道可以直接返回LLRs，这有助于信道解码器更好地估计传输的信息。LLR是两个事件发生概率的对数比值。\n",
    "\n",
    "### Z信道的数学定义：\n",
    "\n",
    "假设有一个简单的Z信道模型，其状态转移可以表示为：\n",
    "\n",
    "- 发送`0`，接收`0`的概率：`1`\n",
    "- 发送`1`，接收`0`的概率：`p`\n",
    "- 发送`1`，接收`1`的概率：`1-p`\n",
    "\n",
    "### Z信道的应用：\n",
    "\n",
    "- **信道编码**：研究如何设计编码方案来最小化Z信道中的错误。\n",
    "- **信号检测**：探讨在Z信道中如何有效地检测和估计传输的信号。\n",
    "- **性能分析**：评估不同信道编码和解码算法在Z信道条件下的性能。\n",
    "\n",
    "Z信道由于其简化的错误模型，为理解和设计鲁棒的通信系统提供了一个有用的理论基础。尽管实际的通信信道可能比Z信道复杂得多，但Z信道仍然是一个重要的教学工具，帮助人们理解信道噪声和错误纠正的基本概念。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BinaryErasureChannel\n",
    "这段代码实现了一个二元擦除信道（Binary Erasure Channel），用于模拟在传输过程中可能发生的擦除错误。以下是对代码的解释和在 PyTorch 中的改写：\n",
    "\n",
    "### 代码解释\n",
    "\n",
    "1. **初始化方法 (`__init__`)**:\n",
    "   - 初始化函数接受一系列参数，包括是否返回 LLRs (`return_llrs`)、是否使用极性输入 (`bipolar_input`)、LLRs 的最大值 (`llr_max`)、数据类型 (`dtype`) 等。\n",
    "   - 在初始化过程中，通过断言检查数据类型 `dtype` 是否是支持的类型之一，排除了无符号整数。\n",
    "\n",
    "2. **`build` 方法**:\n",
    "   - 在 TensorFlow 的习惯中，`build` 方法通常用于验证输入形状或构建层。这里的 `build` 方法没有实际操作，因为 PyTorch 中的层通常不需要显式构建。\n",
    "\n",
    "3. **`call` 方法**:\n",
    "   - `call` 方法实现了如何应用二元擦除信道到输入数据。\n",
    "   - 首先将概率 `pb` 强制转换为 `tf.float32` 类型，并使用 `tf.clip_by_value` 对其进行了数值稳定性的裁剪，保证在 [0, 1] 范围内。\n",
    "   - 调用 `_check_inputs` 方法检查输入 `x` 是否符合预期（二元或极性）。\n",
    "   - 使用 `_sample_errors` 方法根据擦除概率 `pb` 采样生成擦除模式 `e`。\n",
    "   - 根据 `_return_llrs` 决定是否返回 LLRs。如果是，则计算 LLRs，并将擦除位置的 LLRs 设置为 0；否则，根据 `_bipolar_input` 决定擦除位置的输出值（0 或 -1）。\n",
    "\n",
    "### 改写成 PyTorch\n",
    "\n",
    "在 PyTorch 中，可以使用类似的逻辑来实现二元擦除信道。以下是将该代码改写成 PyTorch 的示例：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# SPDX-FileCopyrightText: Copyright (c) 2021-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#\n",
    "\"\"\"Layer for discrete channel models\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from sionna.utils import expand_to_rank\n",
    "\n",
    "from sionna.constants import GLOBAL_SEED_NUMBER\n",
    "\n",
    "class BinaryMemorylessChannel(Layer):\n",
    "    # pylint: disable=line-too-long\n",
    "    r\"\"\"BinaryMemorylessChannel(return_llrs=False, bipolar_input=False, llr_max=100., dtype=tf.float32, **kwargs)\n",
    "\n",
    "    Discrete binary memory less channel with (possibly) asymmetric bit flipping\n",
    "    probabilities.\n",
    "\n",
    "    Inputs bits are flipped with probability :math:`p_\\text{b,0}` and\n",
    "    :math:`p_\\text{b,1}`, respectively.\n",
    "\n",
    "    ..  figure:: ../figures/BMC_channel.png\n",
    "        :align: center\n",
    "\n",
    "    This layer supports binary inputs (:math:`x \\in \\{0, 1\\}`) and `bipolar`\n",
    "    inputs (:math:`x \\in \\{-1, 1\\}`).\n",
    "\n",
    "    If activated, the channel directly returns log-likelihood ratios (LLRs)\n",
    "    defined as\n",
    "\n",
    "    .. math::\n",
    "        \\ell =\n",
    "        \\begin{cases}\n",
    "            \\operatorname{log} \\frac{p_{b,1}}{1-p_{b,0}}, \\qquad \\text{if} \\, y=0 \\\\\n",
    "            \\operatorname{log} \\frac{1-p_{b,1}}{p_{b,0}}, \\qquad \\text{if} \\, y=1 \\\\\n",
    "        \\end{cases}\n",
    "\n",
    "    The error probability :math:`p_\\text{b}` can be either scalar or a\n",
    "    tensor (broadcastable to the shape of the input). This allows\n",
    "    different erasure probabilities per bit position. In any case, its last\n",
    "    dimension must be of length 2 and is interpreted as :math:`p_\\text{b,0}` and\n",
    "    :math:`p_\\text{b,1}`.\n",
    "\n",
    "    This class inherits from the Keras `Layer` class and can be used as layer in\n",
    "    a Keras model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    return_llrs: bool\n",
    "        Defaults to `False`. If `True`, the layer returns log-likelihood ratios\n",
    "        instead of binary values based on ``pb``.\n",
    "\n",
    "    bipolar_input : bool, False\n",
    "        Defaults to `False`. If `True`, the expected input is given as\n",
    "        :math:`\\{-1,1\\}` instead of :math:`\\{0,1\\}`.\n",
    "\n",
    "    llr_max: tf.float\n",
    "        Defaults to 100. Defines the clipping value of the LLRs.\n",
    "\n",
    "    dtype : tf.DType\n",
    "        Defines the datatype for internal calculations and the output\n",
    "        dtype. Defaults to `tf.float32`.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    (x, pb) :\n",
    "        Tuple:\n",
    "\n",
    "    x : [...,n], tf.float32\n",
    "        Input sequence to the channel consisting of binary values :math:`\\{0,1\\}\n",
    "        ` or :math:`\\{-1,1\\}`, respectively.\n",
    "\n",
    "    pb : [...,2], tf.float32\n",
    "        Error probability. Can be a tuple of two scalars or of any\n",
    "        shape that can be broadcasted to the shape of ``x``. It has an\n",
    "        additional last dimension which is interpreted as :math:`p_\\text{b,0}`\n",
    "        and :math:`p_\\text{b,1}`.\n",
    "\n",
    "    Output\n",
    "    -------\n",
    "        : [...,n], tf.float32\n",
    "            Output sequence of same length as the input ``x``. If\n",
    "            ``return_llrs`` is `False`, the output is ternary where a `-1` and\n",
    "            `0` indicate an erasure for the binary and bipolar input,\n",
    "            respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_llrs=False, bipolar_input=False, llr_max=100.,dtype=tf.float32, **kwargs):\n",
    "\n",
    "        super().__init__(dtype=dtype,**kwargs)\n",
    "\n",
    "        assert isinstance(return_llrs, bool), \"return_llrs must be bool.\"\n",
    "        self._return_llrs = return_llrs\n",
    "\n",
    "        assert isinstance(bipolar_input, bool), \"bipolar_input must be bool.\"\n",
    "        self._bipolar_input = bipolar_input\n",
    "\n",
    "        assert llr_max>=0., \"llr_max must be a positive scalar value.\"\n",
    "        self._llr_max = tf.cast(llr_max, dtype=self.dtype)\n",
    "\n",
    "        if self._return_llrs:\n",
    "            assert dtype in (tf.float16, tf.float32, tf.float64),\\\n",
    "                \"LLR outputs require non-integer dtypes.\"\n",
    "        else:\n",
    "            if self._bipolar_input:\n",
    "                assert dtype in (tf.float16, tf.float32, tf.float64,\n",
    "                    tf.int8, tf.int16, tf.int32, tf.int64),\\\n",
    "                    \"Only, signed dtypes are supported for bipolar inputs.\"\n",
    "            else:\n",
    "                assert dtype in (tf.float16, tf.float32, tf.float64,\n",
    "                    tf.uint8, tf.uint16, tf.uint32, tf.uint64,\n",
    "                    tf.int8, tf.int16, tf.int32, tf.int64),\\\n",
    "                    \"Only, real-valued dtypes are supported.\"\n",
    "\n",
    "        self._check_input = True # check input for consistency (i.e., binary)\n",
    "\n",
    "        self._eps = 1e-9 # small additional term for numerical stability\n",
    "        self._temperature = tf.constant(0.1, tf.float32) # for Gumble-softmax\n",
    "\n",
    "    #########################################\n",
    "    # Public methods and properties\n",
    "    #########################################\n",
    "\n",
    "    @property\n",
    "    def llr_max(self):\n",
    "        \"\"\"Maximum value used for LLR calculations.\"\"\"\n",
    "        return self._llr_max\n",
    "\n",
    "    @llr_max.setter\n",
    "    def llr_max(self, value):\n",
    "        \"\"\"Maximum value used for LLR calculations.\"\"\"\n",
    "        assert value>=0, 'llr_max cannot be negative.'\n",
    "        self._llr_max = tf.cast(value, dtype=tf.float32)\n",
    "\n",
    "    @property\n",
    "    def temperature(self):\n",
    "        \"\"\"Temperature for Gumble-softmax trick.\"\"\"\n",
    "        return self._temperature\n",
    "\n",
    "    @temperature.setter\n",
    "    def temperature(self, value):\n",
    "        \"\"\"Temperature for Gumble-softmax trick.\"\"\"\n",
    "        assert value>=0, 'temperature cannot be negative.'\n",
    "        self._temperature = tf.cast(value, dtype=tf.float32)\n",
    "\n",
    "    #########################\n",
    "    # Utility methods\n",
    "    #########################\n",
    "\n",
    "    def _check_inputs(self, x):\n",
    "        \"\"\"Check input x for consistency, i.e., verify\n",
    "        that all values are binary of bipolar values.\"\"\"\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        if self._check_input:\n",
    "            if self._bipolar_input: # allow -1 and 1 for bipolar inputs\n",
    "                values = (tf.constant(-1, x.dtype),tf.constant(1, x.dtype))\n",
    "            else: # allow 0,1 for binary input\n",
    "                values = (tf.constant(0, x.dtype),tf.constant(1, x.dtype))\n",
    "            tf.debugging.assert_equal(\n",
    "                tf.reduce_min(tf.cast(tf.logical_or(tf.equal(x, values[0]),\n",
    "                                    tf.equal(x, values[1])), x.dtype)),\n",
    "                tf.constant(1, x.dtype),\n",
    "                \"Input must be binary.\")\n",
    "            # input datatype consistency should be only evaluated once\n",
    "            self._check_input = False\n",
    "\n",
    "    @tf.custom_gradient\n",
    "    def _custom_xor(self, a, b):\n",
    "        \"\"\"Straight through estimator for XOR.\"\"\"\n",
    "        def grad(upstream):\n",
    "            \"\"\"identity in backward direction\"\"\"\n",
    "            return upstream, upstream\n",
    "        # xor in forward path\n",
    "        # use module for \"exotic\" dtypes\n",
    "        if self.dtype in (tf.uint8, tf.uint16, tf.uint32, tf.uint64, tf.int8, tf.int16, tf.int32, tf.int64):\n",
    "            z = tf.math.mod(a+b, tf.constant(2, self.dtype))\n",
    "        else: # use abs for float dtypes\n",
    "            z = tf.abs(a - b)\n",
    "\n",
    "        return z, grad\n",
    "\n",
    "    @tf.custom_gradient\n",
    "    def _ste_binarizer(self, x):\n",
    "        \"\"\"Straight through binarizer to quantize bits to int values.\"\"\"\n",
    "        def grad(upstream):\n",
    "            \"\"\"identity in backward direction\"\"\"\n",
    "            return upstream\n",
    "        # hard-decide in forward path\n",
    "        z = tf.where(x<.5, 0., 1.)\n",
    "        return z, grad\n",
    "\n",
    "    def _sample_errors(self, pb, shape):\n",
    "        \"\"\"Samples binary error vector with given error probability e.\n",
    "        This function is based on the Gumble-softmax \"trick\" to keep the\n",
    "        sampling differentiable.\"\"\"\n",
    "\n",
    "        # this implementation follows https://arxiv.org/pdf/1611.01144v5.pdf\n",
    "        # and https://arxiv.org/pdf/1906.07748.pdf\n",
    "        tf.random.set_seed(GLOBAL_SEED_NUMBER)\n",
    "        u1 = tf.random.uniform(shape=shape,\n",
    "                                minval=0.,\n",
    "                                maxval=1.,\n",
    "                                dtype=tf.float32)\n",
    "        u2 = tf.random.uniform(shape=shape,\n",
    "                                minval=0.,\n",
    "                                maxval=1.,\n",
    "                                dtype=tf.float32)       \n",
    "\n",
    "        u = tf.stack((u1, u2), axis=-1)\n",
    "\n",
    "        # sample Gumble distribution\n",
    "        q = - tf.math.log(- tf.math.log(u + self._eps) + self._eps)\n",
    "        p = tf.stack((pb,1-pb), axis=-1)\n",
    "        p = expand_to_rank(p, tf.rank(q), axis=0)\n",
    "        p = tf.broadcast_to(p, tf.shape(q))\n",
    "        a = (tf.math.log(p + self._eps) + q) / self._temperature\n",
    "\n",
    "        # apply softmax\n",
    "        e_cat = tf.nn.softmax(a)\n",
    "\n",
    "        # binarize final values via straight-through estimator\n",
    "        return self._ste_binarizer(e_cat[...,0]) # only take first class\n",
    "\n",
    "    #########################\n",
    "    # Keras layer functions\n",
    "    #########################\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        \"\"\"Verify correct input shapes\"\"\"\n",
    "\n",
    "        pb_shapes = input_shapes[1]\n",
    "        # allow tuple of scalars as alternative input\n",
    "        if isinstance(pb_shapes, (tuple, list)):\n",
    "            if not len(pb_shapes)==2:\n",
    "                raise ValueError(\"Last dim of pb must be of length 2.\")\n",
    "        else:\n",
    "            if len(pb_shapes)>0:\n",
    "                if not pb_shapes[-1]==2:\n",
    "                    raise ValueError(\"Last dim of pb must be of length 2.\")\n",
    "            else:\n",
    "                raise ValueError(\"Last dim of pb must be of length 2.\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Apply discrete binary memoryless channel to inputs.\"\"\"\n",
    "\n",
    "        x, pb = inputs\n",
    "\n",
    "        # allow pb to be a tuple of two scalars\n",
    "        if isinstance(pb, (tuple, list)):\n",
    "            pb0 = pb[0]\n",
    "            pb1 = pb[1]\n",
    "        else:\n",
    "            pb0 = pb[...,0]\n",
    "            pb1 = pb[...,1]\n",
    "\n",
    "        # clip for numerical stability\n",
    "        pb0 = tf.cast(pb0, tf.float32) # Gumble requires float dtypes\n",
    "        pb1 = tf.cast(pb1, tf.float32) # Gumble requires float dtypes\n",
    "        pb0 = tf.clip_by_value(pb0, 0., 1.)\n",
    "        pb1 = tf.clip_by_value(pb1, 0., 1.)\n",
    "\n",
    "        # check x for consistency (binary, bipolar)\n",
    "        self._check_inputs(x)\n",
    "\n",
    "        e0 = self._sample_errors(pb0, tf.shape(x))\n",
    "        e1 = self._sample_errors(pb1, tf.shape(x))\n",
    "\n",
    "        if self._bipolar_input:\n",
    "            neutral_element = tf.constant(-1, dtype=x.dtype)\n",
    "        else:\n",
    "            neutral_element = tf.constant(0, dtype=x.dtype)\n",
    "\n",
    "        # mask e0 and e1 with input such that e0 only applies where x==0\n",
    "        e = tf.where(x==neutral_element, e0, e1)\n",
    "        e = tf.cast(e, x.dtype)\n",
    "\n",
    "        if self._bipolar_input:\n",
    "            # flip signs for bipolar case\n",
    "            y = x * (-2*e + 1)\n",
    "        else:\n",
    "            # XOR for binary case\n",
    "            y = self._custom_xor(x, e)\n",
    "\n",
    "        # if LLRs should be returned\n",
    "        if self._return_llrs:\n",
    "            if not self._bipolar_input:\n",
    "                y = 2 * y - 1 # transform to bipolar\n",
    "\n",
    "            # Remark: Sionna uses the logit definition log[p(x=1)/p(x=0)]\n",
    "            y0 = - (tf.math.log(pb1 + self._eps)\n",
    "                   - tf.math.log(1 - pb0 - self._eps))\n",
    "            y1 = (tf.math.log(1 - pb1 - self._eps)\n",
    "                  - tf.math.log(pb0 + self._eps))\n",
    "            # multiply by y to keep gradient\n",
    "            y = tf.cast(tf.where(y==1, y1, y0), dtype=y.dtype) * y\n",
    "            # and clip output llrs\n",
    "            y = tf.clip_by_value(y, -self._llr_max, self._llr_max)\n",
    "\n",
    "        return y\n",
    "\n",
    "class BinarySymmetricChannel(BinaryMemorylessChannel):\n",
    "    # pylint: disable=line-too-long\n",
    "    r\"\"\"BinarySymmetricChannel(return_llrs=False, bipolar_input=False, llr_max=100., dtype=tf.float32, **kwargs)\n",
    "\n",
    "    Discrete binary symmetric channel which randomly flips bits with probability\n",
    "    :math:`p_\\text{b}`.\n",
    "\n",
    "    ..  figure:: ../figures/BSC_channel.png\n",
    "        :align: center\n",
    "\n",
    "    This layer supports binary inputs (:math:`x \\in \\{0, 1\\}`) and `bipolar`\n",
    "    inputs (:math:`x \\in \\{-1, 1\\}`).\n",
    "\n",
    "    If activated, the channel directly returns log-likelihood ratios (LLRs)\n",
    "    defined as\n",
    "\n",
    "    .. math::\n",
    "        \\ell =\n",
    "        \\begin{cases}\n",
    "            \\operatorname{log} \\frac{p_{b}}{1-p_{b}}, \\qquad \\text{if}\\, y=0 \\\\\n",
    "            \\operatorname{log} \\frac{1-p_{b}}{p_{b}}, \\qquad \\text{if}\\, y=1 \\\\\n",
    "        \\end{cases}\n",
    "    where :math:`y` denotes the binary output of the channel.\n",
    "\n",
    "    The bit flipping probability :math:`p_\\text{b}` can be either a scalar or  a\n",
    "    tensor (broadcastable to the shape of the input). This allows\n",
    "    different bit flipping probabilities per bit position.\n",
    "\n",
    "    This class inherits from the Keras `Layer` class and can be used as layer in\n",
    "    a Keras model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    return_llrs: bool\n",
    "        Defaults to `False`. If `True`, the layer returns log-likelihood ratios\n",
    "        instead of binary values based on ``pb``.\n",
    "\n",
    "    bipolar_input : bool, False\n",
    "        Defaults to `False`. If `True`, the expected input is given as {-1,1}\n",
    "        instead of {0,1}.\n",
    "\n",
    "    llr_max: tf.float\n",
    "        Defaults to 100. Defines the clipping value of the LLRs.\n",
    "\n",
    "    dtype : tf.DType\n",
    "        Defines the datatype for internal calculations and the output\n",
    "        dtype. Defaults to `tf.float32`.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    (x, pb) :\n",
    "        Tuple:\n",
    "\n",
    "    x : [...,n], tf.float32\n",
    "        Input sequence to the channel.\n",
    "\n",
    "    pb : tf.float32\n",
    "        Bit flipping probability. Can be a scalar or of any shape that\n",
    "        can be broadcasted to the shape of ``x``.\n",
    "\n",
    "    Output\n",
    "    -------\n",
    "        : [...,n], tf.float32\n",
    "            Output sequence of same length as the input ``x``. If\n",
    "            ``return_llrs`` is `False`, the output is binary and otherwise\n",
    "            soft-values are returned.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_llrs=False, bipolar_input=False, llr_max=100., dtype=tf.float32, **kwargs):\n",
    "\n",
    "        super().__init__(return_llrs=return_llrs,\n",
    "                         bipolar_input=bipolar_input,\n",
    "                         llr_max=llr_max,\n",
    "                         dtype=dtype,\n",
    "                         **kwargs)\n",
    "\n",
    "    #########################\n",
    "    # Keras layer functions\n",
    "    #########################\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        \"\"\"Verify correct input shapes\"\"\"\n",
    "        pass # nothing to verify here\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Apply discrete binary symmetric channel, i.e., randomly flip\n",
    "        bits with probability pb.\"\"\"\n",
    "\n",
    "        x, pb = inputs\n",
    "\n",
    "        # the BSC is implemented by calling the DMC with symmetric pb\n",
    "        pb = tf.cast(pb, x.dtype)\n",
    "        pb = tf.stack((pb, pb), axis=-1)\n",
    "        y = super().call((x, pb))\n",
    "\n",
    "        return y\n",
    "\n",
    "class BinaryZChannel(BinaryMemorylessChannel):\n",
    "    # pylint: disable=line-too-long\n",
    "    r\"\"\"BinaryZChannel(return_llrs=False, bipolar_input=False, llr_max=100., dtype=tf.float32, **kwargs)\n",
    "\n",
    "    Layer that implements the binary Z-channel.\n",
    "\n",
    "    In the Z-channel, transmission errors only occur for the transmission of\n",
    "    second input element (i.e., if a `1` is transmitted) with error probability\n",
    "    probability :math:`p_\\text{b}` but the first element is always correctly\n",
    "    received.\n",
    "\n",
    "    ..  figure:: ../figures/Z_channel.png\n",
    "        :align: center\n",
    "\n",
    "\n",
    "    This layer supports binary inputs (:math:`x \\in \\{0, 1\\}`) and `bipolar`\n",
    "    inputs (:math:`x \\in \\{-1, 1\\}`).\n",
    "\n",
    "    If activated, the channel directly returns log-likelihood ratios (LLRs)\n",
    "    defined as\n",
    "\n",
    "    .. math::\n",
    "        \\ell =\n",
    "        \\begin{cases}\n",
    "            \\operatorname{log} \\left( p_b \\right), \\qquad \\text{if} \\, y=0 \\\\\n",
    "            \\infty, \\qquad \\qquad \\text{if} \\, y=1 \\\\\n",
    "        \\end{cases}\n",
    "    assuming equal probable inputs :math:`P(X=0) = P(X=1) = 0.5`.\n",
    "\n",
    "    The error probability :math:`p_\\text{b}` can be either a scalar or a\n",
    "    tensor (broadcastable to the shape of the input). This allows\n",
    "    different error probabilities per bit position.\n",
    "\n",
    "    This class inherits from the Keras `Layer` class and can be used as layer in\n",
    "    a Keras model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    return_llrs: bool\n",
    "        Defaults to `False`. If `True`, the layer returns log-likelihood ratios\n",
    "        instead of binary values based on ``pb``.\n",
    "\n",
    "    bipolar_input : bool, False\n",
    "        Defaults to `False`. If True, the expected input is given as {-1,1}\n",
    "        instead of {0,1}.\n",
    "\n",
    "    llr_max: tf.float\n",
    "        Defaults to 100. Defines the clipping value of the LLRs.\n",
    "\n",
    "    dtype : tf.DType\n",
    "        Defines the datatype for internal calculations and the output\n",
    "        dtype. Defaults to `tf.float32`.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    (x, pb) :\n",
    "        Tuple:\n",
    "\n",
    "    x : [...,n], tf.float32\n",
    "        Input sequence to the channel.\n",
    "\n",
    "    pb : tf.float32\n",
    "        Error probability. Can be a scalar or of any shape that can be\n",
    "        broadcasted to the shape of ``x``.\n",
    "\n",
    "    Output\n",
    "    -------\n",
    "        : [...,n], tf.float32\n",
    "            Output sequence of same length as the input ``x``. If\n",
    "            ``return_llrs`` is `False`, the output is binary and otherwise\n",
    "            soft-values are returned.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_llrs=False, bipolar_input=False, llr_max=100.,dtype=tf.float32, **kwargs):\n",
    "\n",
    "        super().__init__(return_llrs=return_llrs,\n",
    "                         bipolar_input=bipolar_input,\n",
    "                         llr_max=llr_max,\n",
    "                         dtype=dtype,\n",
    "                         **kwargs)\n",
    "\n",
    "    #########################\n",
    "    # Keras layer functions\n",
    "    #########################\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        \"\"\"Verify correct input shapes\"\"\"\n",
    "        pass # nothing to verify here\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Apply discrete binary symmetric channel, i.e., randomly flip\n",
    "        bits with probability pb.\"\"\"\n",
    "\n",
    "        x, pb = inputs\n",
    "\n",
    "        # the Z is implemented by calling the DMC with p(1|0)=0\n",
    "        pb = tf.cast(pb, x.dtype)\n",
    "        pb = tf.stack((tf.zeros_like(pb), pb), axis=-1)\n",
    "        y = super().call((x, pb))\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class BinaryErasureChannel(BinaryMemorylessChannel):\n",
    "    # pylint: disable=line-too-long\n",
    "    r\"\"\"BinaryErasureChannel(return_llrs=False, bipolar_input=False, llr_max=100., dtype=tf.float32, **kwargs)\n",
    "\n",
    "    Binary erasure channel (BEC) where a bit is either correctly received\n",
    "    or erased.\n",
    "\n",
    "    In the binary erasure channel, bits are always correctly received or erased\n",
    "    with erasure probability :math:`p_\\text{b}`.\n",
    "\n",
    "    ..  figure:: ../figures/BEC_channel.png\n",
    "        :align: center\n",
    "\n",
    "    This layer supports binary inputs (:math:`x \\in \\{0, 1\\}`) and `bipolar`\n",
    "    inputs (:math:`x \\in \\{-1, 1\\}`).\n",
    "\n",
    "    If activated, the channel directly returns log-likelihood ratios (LLRs)\n",
    "    defined as\n",
    "\n",
    "    .. math::\n",
    "        \\ell =\n",
    "        \\begin{cases}\n",
    "            -\\infty, \\qquad \\text{if} \\, y=0 \\\\\n",
    "            0, \\qquad \\quad \\,\\, \\text{if} \\, y=? \\\\\n",
    "            \\infty, \\qquad \\quad \\text{if} \\, y=1 \\\\\n",
    "        \\end{cases}\n",
    "\n",
    "    The erasure probability :math:`p_\\text{b}` can be either a scalar or a\n",
    "    tensor (broadcastable to the shape of the input). This allows\n",
    "    different erasure probabilities per bit position.\n",
    "\n",
    "    Please note that the output of the BEC is ternary. Hereby, `-1` indicates an\n",
    "    erasure for the binary configuration and `0` for the bipolar mode,\n",
    "    respectively.\n",
    "\n",
    "    This class inherits from the Keras `Layer` class and can be used as layer in\n",
    "    a Keras model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    return_llrs: bool\n",
    "        Defaults to `False`. If `True`, the layer returns log-likelihood ratios\n",
    "        instead of binary values based on ``pb``.\n",
    "\n",
    "    bipolar_input : bool, False\n",
    "        Defaults to `False`. If `True`, the expected input is given as {-1,1}\n",
    "        instead of {0,1}.\n",
    "\n",
    "    llr_max: tf.float\n",
    "        Defaults to 100. Defines the clipping value of the LLRs.\n",
    "\n",
    "    dtype : tf.DType\n",
    "        Defines the datatype for internal calculations and the output\n",
    "        dtype. Defaults to `tf.float32`.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    (x, pb) :\n",
    "        Tuple:\n",
    "\n",
    "    x : [...,n], tf.float32\n",
    "        Input sequence to the channel.\n",
    "\n",
    "    pb : tf.float32\n",
    "        Erasure probability. Can be a scalar or of any shape that can be\n",
    "        broadcasted to the shape of ``x``.\n",
    "\n",
    "    Output\n",
    "    -------\n",
    "        : [...,n], tf.float32\n",
    "            Output sequence of same length as the input ``x``. If\n",
    "            ``return_llrs`` is `False`, the output is ternary where each `-1`\n",
    "            and each `0` indicate an erasure for the binary and bipolar input,\n",
    "            respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_llrs=False, bipolar_input=False, llr_max=100.,dtype=tf.float32, **kwargs):\n",
    "\n",
    "        super().__init__(return_llrs=return_llrs,\n",
    "                         bipolar_input=bipolar_input,\n",
    "                         llr_max=llr_max,\n",
    "                         dtype=dtype,\n",
    "                         **kwargs)\n",
    "\n",
    "        # also exclude uints, as -1 indicator for erasures does not exist\n",
    "        assert dtype in (tf.float16, tf.float32, tf.float64,\n",
    "                tf.int8, tf.int16, tf.int32, tf.int64),\\\n",
    "                \"Unsigned integers are currently not supported.\"\n",
    "\n",
    "    #########################\n",
    "    # Keras layer functions\n",
    "    #########################\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        \"\"\"Verify correct input shapes\"\"\"\n",
    "        pass # nothing to verify here\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Apply erasure channel to inputs.\"\"\"\n",
    "\n",
    "        x, pb = inputs\n",
    "\n",
    "        # clip for numerical stability\n",
    "        pb = tf.cast(pb, tf.float32) # Gumble requires float dtypes\n",
    "        pb = tf.clip_by_value(pb, 0., 1.)\n",
    "\n",
    "        # check x for consistency (binary, bipolar)\n",
    "        self._check_inputs(x)\n",
    "\n",
    "        # sample erasure pattern\n",
    "        e = self._sample_errors(pb, tf.shape(x))\n",
    "\n",
    "        # if LLRs should be returned\n",
    "        # remark: the Sionna logit definition is llr = log[p(x=1)/p(x=0)]\n",
    "        if self._return_llrs:\n",
    "            if not self._bipolar_input:\n",
    "                x = 2 * x -1\n",
    "            x *= tf.cast(self._llr_max, x.dtype) # calculate llrs\n",
    "\n",
    "            # erase positions by setting llrs to 0\n",
    "            y = tf.where(e==1, tf.constant(0, x.dtype), x)\n",
    "        else: # ternary outputs\n",
    "            # the erasure indicator depends on the operation mode\n",
    "            if self._bipolar_input:\n",
    "                erased_element = tf.constant(0, dtype=x.dtype)\n",
    "            else:\n",
    "                erased_element = tf.constant(-1, dtype=x.dtype)\n",
    "\n",
    "            y = tf.where(e==0, x, erased_element)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据 x:\n",
      "tf.Tensor(\n",
      "[[0. 1. 0. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [1. 1. 0. 0.]], shape=(4, 4), dtype=float32)\n",
      "\n",
      "翻转概率 pb:\n",
      "tf.Tensor(\n",
      "[[0.1 0.9]\n",
      " [0.5 0.5]\n",
      " [0.2 0.8]\n",
      " [0.8 0.2]], shape=(4, 2), dtype=float32)\n",
      "\n",
      "输出数据 y:\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 测试 BinaryMemorylessChannel 函数\n",
    "def test_binary_memoryless_channel():\n",
    "    # Define more complex input data\n",
    "    x = tf.constant([[0, 1, 0, 1], [1, 0, 1, 0], [0, 0, 1, 1], [1, 1, 0, 0]], dtype=np.float32)\n",
    "    pb = tf.constant([[0.1, 0.9], [0.5, 0.5], [0.2, 0.8], [0.8, 0.2]], dtype=np.float32)\n",
    "\n",
    "    # Create instance of BinaryMemorylessChannel\n",
    "    bmc = BinaryMemorylessChannel(return_llrs=True, bipolar_input=False, llr_max=100.)\n",
    "\n",
    "    # Perform forward pass\n",
    "    y = bmc((x, pb))\n",
    "\n",
    "    # Print input and output\n",
    "    print(\"输入数据 x:\")\n",
    "    print(x)\n",
    "    print(\"\\n翻转概率 pb:\")\n",
    "    print(pb)\n",
    "    print(\"\\n输出数据 y:\")\n",
    "    print(y)\n",
    "\n",
    "# Run the test function\n",
    "test_binary_memoryless_channel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据 x:\n",
      "[[0. 1. 0. 1.]\n",
      " [1. 0. 1. 0.]]\n",
      "\n",
      "翻转概率 pb:\n",
      "0.37\n",
      "\n",
      "输出数据 y:\n",
      "[[0. 0. 0. 0.]\n",
      " [1. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 测试 BinarySymmetricChannel 类\n",
    "# 定义输入数据\n",
    "x = tf.constant([[0, 1, 0, 1], [1, 0, 1, 0]], dtype=tf.float32)\n",
    "pb = tf.constant(0.37, dtype=tf.float32)  # 100% 概率翻转\n",
    "\n",
    "# 创建 BinarySymmetricChannel 实例\n",
    "bsc = BinarySymmetricChannel()\n",
    "\n",
    "# 执行前向传播\n",
    "y = bsc((x, pb))\n",
    "\n",
    "# 打印输入和输出\n",
    "print(\"输入数据 x:\")\n",
    "print(x.numpy())\n",
    "print(\"\\n翻转概率 pb:\")\n",
    "print(pb.numpy())\n",
    "print(\"\\n输出数据 y:\")\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据 x:\n",
      "[[0. 1. 0. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [1. 1. 0. 0.]]\n",
      "\n",
      "翻转概率 pb:\n",
      "[0.1 0.3 0.2 0.8]\n",
      "\n",
      "输出数据 y:\n",
      "[[0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 测试 BinaryZChannel 类\n",
    "# 定义更复杂的输入数据\n",
    "x = tf.constant([[0, 1, 0, 1], [1, 0, 1, 0], [0, 0, 1, 1], [1, 1, 0, 0]], dtype=tf.float32)\n",
    "pb = tf.constant([0.1, 0.3, 0.2, 0.8], dtype=tf.float32)  # 各种不同的概率\n",
    "\n",
    "# 创建 BinaryZChannel 实例\n",
    "bzc = BinaryZChannel()\n",
    "\n",
    "# 执行前向传播\n",
    "y = bzc((x, pb))\n",
    "\n",
    "# 打印输入和输出\n",
    "print(\"输入数据 x:\")\n",
    "print(x.numpy())\n",
    "print(\"\\n翻转概率 pb:\")\n",
    "print(pb.numpy())\n",
    "print(\"\\n输出数据 y:\")\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据 input_data:\n",
      "[-1.  1. -1.  1.  1.]\n",
      "\n",
      "翻转概率 pb:\n",
      "0.48\n",
      "\n",
      "输出数据 output:\n",
      "[-1.  0. -1.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "# 测试BinaryErasureChannel类\n",
    "# Usage example\n",
    "input_data = tf.constant([-1, 1, -1, 1, 1], dtype=tf.float32)\n",
    "pb = tf.constant(0.48)\n",
    "channel = BinaryErasureChannel(return_llrs=False, bipolar_input=True)\n",
    "\n",
    "output = channel((input_data, pb))\n",
    "# 打印输入和输出\n",
    "print(\"输入数据 input_data:\")\n",
    "print(input_data.numpy())\n",
    "print(\"\\n翻转概率 pb:\")\n",
    "print(pb.numpy())\n",
    "print(\"\\n输出数据 output:\")\n",
    "print(output.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mysionna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
